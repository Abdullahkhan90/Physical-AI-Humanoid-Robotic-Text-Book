"use strict";(globalThis.webpackChunkai_textbook_docusaurus=globalThis.webpackChunkai_textbook_docusaurus||[]).push([[963],{2918:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var a=t(4848),i=t(8453);const o={sidebar_position:3,title:"Cognitive Planning with LLMs"},s="Cognitive Planning: Using LLMs to Translate Natural Language into ROS 2 Actions",r={id:"module-4-vla/cognitive-planning",title:"Cognitive Planning with LLMs",description:"Large Language Models (LLMs) have emerged as powerful tools for natural language understanding and generation. In robotics, they can serve as high-level cognitive planners that interpret natural language commands and translate them into sequences of executable robot actions. This section explores how to integrate LLMs into robotic systems for cognitive planning.",source:"@site/docs/module-4-vla/cognitive-planning.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/cognitive-planning",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/cognitive-planning",draft:!1,unlisted:!1,editUrl:"https://github.com/AI-Textbook-Project/ai-textbook-physical-ai-humanoid/edit/main/docs/module-4-vla/cognitive-planning.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Cognitive Planning with LLMs"},sidebar:"tutorialSidebar",previous:{title:"Voice-to-Action with OpenAI Whisper",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/voice-to-action"},next:{title:"Capstone Project - The Autonomous Humanoid",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/capstone-project"}},l={},c=[{value:"Introduction to Cognitive Planning in Robotics",id:"introduction-to-cognitive-planning-in-robotics",level:2},{value:"Role of LLMs in Cognitive Planning",id:"role-of-llms-in-cognitive-planning",level:2},{value:"Advantages of LLMs for Planning",id:"advantages-of-llms-for-planning",level:3},{value:"Limitations and Mitigations",id:"limitations-and-mitigations",level:3},{value:"Architecture for LLM-Based Cognitive Planning",id:"architecture-for-llm-based-cognitive-planning",level:2},{value:"System Components",id:"system-components",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:3},{value:"Implementation Approaches",id:"implementation-approaches",level:2},{value:"Prompt Engineering",id:"prompt-engineering",level:3},{value:"Example Implementation",id:"example-implementation",level:3},{value:"Integration with Robot Control Systems",id:"integration-with-robot-control-systems",level:2},{value:"ROS 2 Action Server Pattern",id:"ros-2-action-server-pattern",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Action Validation",id:"action-validation",level:3},{value:"Performance and Efficiency Optimization",id:"performance-and-efficiency-optimization",level:2},{value:"Caching",id:"caching",level:3},{value:"Local Models",id:"local-models",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Plan Failure Handling",id:"plan-failure-handling",level:3},{value:"Evaluation and Improvement",id:"evaluation-and-improvement",level:2},{value:"Metrics for Cognitive Planning",id:"metrics-for-cognitive-planning",level:3},{value:"Continuous Learning",id:"continuous-learning",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"cognitive-planning-using-llms-to-translate-natural-language-into-ros-2-actions",children:"Cognitive Planning: Using LLMs to Translate Natural Language into ROS 2 Actions"}),"\n",(0,a.jsx)(e.p,{children:"Large Language Models (LLMs) have emerged as powerful tools for natural language understanding and generation. In robotics, they can serve as high-level cognitive planners that interpret natural language commands and translate them into sequences of executable robot actions. This section explores how to integrate LLMs into robotic systems for cognitive planning."}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-cognitive-planning-in-robotics",children:"Introduction to Cognitive Planning in Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Cognitive planning in robotics involves high-level reasoning that bridges natural language commands to low-level robot actions. It requires:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding natural language commands in context"}),"\n",(0,a.jsx)(e.li,{children:"Breaking down complex tasks into manageable steps"}),"\n",(0,a.jsx)(e.li,{children:"Maintaining situational awareness of the environment"}),"\n",(0,a.jsx)(e.li,{children:"Generating executable sequences of robot actions"}),"\n",(0,a.jsx)(e.li,{children:"Handling unexpected situations and exceptions"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"role-of-llms-in-cognitive-planning",children:"Role of LLMs in Cognitive Planning"}),"\n",(0,a.jsx)(e.h3,{id:"advantages-of-llms-for-planning",children:"Advantages of LLMs for Planning"}),"\n",(0,a.jsx)(e.p,{children:"LLMs bring several advantages to cognitive planning:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Commonsense Reasoning"}),": LLMs possess general world knowledge that can inform action planning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Natural Language Understanding"}),": Direct interpretation of human commands without specialized grammars"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Flexibility"}),": Ability to handle varied and novel instruction formats"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Context Awareness"}),": Understanding of temporal and spatial relationships"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"limitations-and-mitigations",children:"Limitations and Mitigations"}),"\n",(0,a.jsx)(e.p,{children:"However, LLMs also present challenges:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Hallucinations"}),": Producing plausible-sounding but incorrect plans"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Lack of Real-Time Awareness"}),": Not aware of current robot state or environment"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Concerns"}),": Potential to generate unsafe action sequences"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Computational Demands"}),": Resource-intensive inference"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"architecture-for-llm-based-cognitive-planning",children:"Architecture for LLM-Based Cognitive Planning"}),"\n",(0,a.jsx)(e.h3,{id:"system-components",children:"System Components"}),"\n",(0,a.jsx)(e.p,{children:"A typical LLM-based cognitive planning system includes:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"[User Command] ->\n[Natural Language Parser] ->\n[LLM Planner] ->\n[Action Sequence Generator] ->\n[Robot Execution Layer] ->\n[Environment Perception] <- Feedback Loop\n"})}),"\n",(0,a.jsx)(e.h3,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,a.jsx)(e.p,{children:"The system typically integrates with ROS 2 through:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Services for high-level command requests"}),"\n",(0,a.jsx)(e.li,{children:"Action servers for long-running tasks"}),"\n",(0,a.jsx)(e.li,{children:"Publishers/subscribers for state monitoring"}),"\n",(0,a.jsx)(e.li,{children:"Transform system for spatial reasoning"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"implementation-approaches",children:"Implementation Approaches"}),"\n",(0,a.jsx)(e.h3,{id:"prompt-engineering",children:"Prompt Engineering"}),"\n",(0,a.jsx)(e.p,{children:"Effective cognitive planning relies heavily on well-engineered prompts:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def create_planning_prompt(user_command, robot_capabilities, environment_state):\n    """\n    Creates a structured prompt for the LLM cognitive planner\n    """\n    prompt = f"""\n    You are a cognitive planner for a robot. Your job is to translate human commands into executable robot actions.\n    \n    Current environment state:\n    {environment_state}\n    \n    Available robot capabilities:\n    {robot_capabilities}\n    \n    Human command: "{user_command}"\n    \n    Provide the sequence of actions to execute the command. Format your response as:\n    1. Action: [action_name] Parameters: [parameters]\n    2. Action: [action_name] Parameters: [parameters]\n    ...\n    \n    Only provide actions that the robot is capable of performing.\n    """\n    \n    return prompt\n'})}),"\n",(0,a.jsx)(e.h3,{id:"example-implementation",children:"Example Implementation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import openai  # or another LLM API\nfrom typing import List, Dict, Any\nimport json\n\nclass LLMBasedCognitivePlanner:\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        self.api_key = api_key\n        self.model = model\n        openai.api_key = api_key\n        \n        # Define known robot capabilities\n        self.known_capabilities = [\n            "move_to_location",\n            "pick_object",\n            "place_object",\n            "navigate",\n            "detect_object",\n            "grasp",\n            "release",\n            "follow_route"\n        ]\n        \n    def plan_action_sequence(self, natural_language_command: str, \n                           robot_state: Dict[str, Any],\n                           environment_state: Dict[str, Any]) -> List[Dict[str, Any]]:\n        """\n        Generate action sequence from natural language command\n        """\n        # Create planning prompt\n        prompt = self._create_planning_prompt(\n            natural_language_command,\n            robot_state,\n            environment_state\n        )\n        \n        # Call LLM\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[{"role": "user", "content": prompt}]\n        )\n        \n        # Parse response into action sequence\n        response_text = response.choices[0].message[\'content\']\n        actions = self._parse_actions(response_text)\n        \n        # Validate actions\n        valid_actions = self._validate_action_sequence(actions)\n        \n        return valid_actions\n    \n    def _create_planning_prompt(self, command: str, \n                                robot_state: Dict[str, Any],\n                                env_state: Dict[str, Any]) -> str:\n        """Create a structured prompt for the LLM"""\n        # Implementation of prompt creation\n        # Include robot capabilities, environment context, etc.\n        prompt = f"""\n        As a cognitive planning system for a robot, translate this natural language command into a sequence of actions:\n        Command: "{command}"\n        \n        Environment context:\n        - Objects: {[obj[\'name\'] for obj in env_state.get(\'objects\', [])]}\n        - Locations: {[loc[\'name\'] for loc in env_state.get(\'locations\', [])]}\n        - Robot position: {robot_state.get(\'position\', \'unknown\')}\n        - Robot battery: {robot_state.get(\'battery_level\', \'unknown\')}%\n        \n        Available actions:\n        - move_to_location: Move robot to a named location\n        - pick_object: Pick up an object by name\n        - place_object: Place held object at location\n        - navigate: Navigate to coordinates\n        - detect_object: Look for an object\n        - grasp: Grasp current object\n        - release: Release current object\n        - follow_route: Follow predefined route\n        \n        Provide the plan as numbered steps in this format:\n        1. move_to_location: location="kitchen"\n        2. detect_object: object="cup"\n        3. pick_object: object="cup"\n        4. move_to_location: location="table"\n        5. place_object: location="table"\n\n        Return only the action sequence, nothing else.\n        """\n        return prompt\n    \n    def _parse_actions(self, response_text: str) -> List[Dict[str, Any]]:\n        """Parse the LLM response into executable actions"""\n        actions = []\n        \n        # Simple regex-based parser (more sophisticated approaches exist)\n        import re\n        \n        # Match pattern like: "1. action_name: param1=value1, param2=value2"\n        pattern = r\'(\\d+)\\.\\s*([a-zA-Z_][a-zA-Z0-9_]*)\\s*:\\s*(.*)\'\n        \n        matches = re.findall(pattern, response_text)\n        \n        for match in matches:\n            step_num, action_name, params_str = match\n            \n            # Parse parameters\n            params = {}\n            if params_str.strip():\n                # Split on commas that are not within quotes\n                param_pairs = [p.strip() for p in params_str.split(\',\')]\n                for pair in param_pairs:\n                    if \'=\' in pair:\n                        key, value = pair.split(\'=\', 1)\n                        key = key.strip()\n                        value = value.strip().strip(\'"\\\'\')\n                        \n                        # Try to convert to appropriate type\n                        if value.lower() == \'true\':\n                            value = True\n                        elif value.lower() == \'false\':\n                            value = False\n                        elif value.isdigit():\n                            value = int(value)\n                        else:\n                            try:\n                                value = float(value)\n                            except ValueError:\n                                pass  # Keep as string\n                        \n                        params[key] = value\n            \n            action = {\n                "name": action_name,\n                "parameters": params,\n                "step_number": int(step_num)\n            }\n            actions.append(action)\n        \n        return actions\n    \n    def _validate_action_sequence(self, action_sequence: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        """Validate that actions are executable by the robot"""\n        valid_actions = []\n        \n        for action in action_sequence:\n            action_name = action[\'name\']\n            \n            if action_name not in self.known_capabilities:\n                print(f"Warning: Unknown action \'{action_name}\' in sequence, skipping")\n                continue\n            \n            # Validate parameters if needed\n            valid_actions.append(action)\n        \n        return valid_actions\n'})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-robot-control-systems",children:"Integration with Robot Control Systems"}),"\n",(0,a.jsx)(e.h3,{id:"ros-2-action-server-pattern",children:"ROS 2 Action Server Pattern"}),"\n",(0,a.jsx)(e.p,{children:"For complex tasks, integrate with ROS 2 action servers:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom your_robot_interfaces.action import ExecuteCommand\nfrom std_msgs.msg import String\n\nclass CognitivePlanningActionServer(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planning_server\')\n        \n        # Initialize the LLM planner\n        self.planner = LLMBasedCognitivePlanner(api_key=your_api_key)\n        \n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            ExecuteCommand,\n            \'execute_command\',\n            self.execute_command_callback)\n        \n        # Robot state and environment publishers/subscribers\n        self.state_publisher = self.create_publisher(String, \'cognitive_planner/state\', 10)\n        \n    def execute_command_callback(self, goal_handle):\n        """Execute a natural language command"""\n        self.get_logger().info(f\'Executing command: {goal_handle.request.command}\')\n        \n        # Get current robot and environment state\n        robot_state = self._get_robot_state()\n        env_state = self._get_environment_state()\n        \n        # Plan action sequence\n        try:\n            action_sequence = self.planner.plan_action_sequence(\n                goal_handle.request.command,\n                robot_state,\n                env_state\n            )\n            \n            # Execute the sequence\n            for i, action in enumerate(action_sequence):\n                # Update feedback\n                feedback_msg = ExecuteCommand.Feedback()\n                feedback_msg.current_step = f"Executing: {action[\'name\']} ({i+1}/{len(action_sequence)})"\n                goal_handle.publish_feedback(feedback_msg)\n                \n                # Execute action\n                success = self._execute_single_action(action)\n                if not success:\n                    goal_handle.abort()\n                    result = ExecuteCommand.Result()\n                    result.success = False\n                    result.message = f"Failed to execute action: {action}"\n                    return result\n        \n            # Complete successfully\n            goal_handle.succeed()\n            result = ExecuteCommand.Result()\n            result.success = True\n            result.message = f"Successfully executed command with {len(action_sequence)} actions"\n            return result\n            \n        except Exception as e:\n            self.get_logger().error(f\'Error executing command: {str(e)}\')\n            goal_handle.abort()\n            result = ExecuteCommand.Result()\n            result.success = False\n            result.message = f"Error during planning: {str(e)}"\n            return result\n    \n    def _execute_single_action(self, action: Dict[str, Any]) -> bool:\n        """Execute a single action using ROS 2 interfaces"""\n        # This would contain the actual ROS 2 service calls and action executions\n        action_name = action[\'name\']\n        params = action[\'parameters\']\n        \n        # Example mapping to ROS 2 services\n        if action_name == "move_to_location":\n            return self._move_to_location(params.get("location"))\n        elif action_name == "pick_object":\n            return self._pick_object(params.get("object"))\n        elif action_name == "place_object":\n            return self._place_object(params.get("location"))\n        # Additional action mappings...\n        \n        return False  # Unknown action\n    \n    def _get_robot_state(self) -> Dict[str, Any]:\n        """Get current robot state"""\n        # Implementation would query robot state topics\n        return {\n            "position": {"x": 0.0, "y": 0.0, "theta": 0.0},\n            "battery_level": 85,\n            "gripper_status": "open"\n        }\n    \n    def _get_environment_state(self) -> Dict[str, Any]:\n        """Get current environment state"""\n        # Implementation would query environment mapping topics\n        return {\n            "objects": [\n                {"name": "cup", "position": {"x": 1.0, "y": 1.0}},\n                {"name": "box", "position": {"x": 2.0, "y": 0.5}}\n            ],\n            "locations": [\n                {"name": "kitchen", "coordinates": {"x": 5.0, "y": 5.0}},\n                {"name": "table", "coordinates": {"x": 3.0, "y": 2.0}}\n            ]\n        }\n'})}),"\n",(0,a.jsx)(e.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,a.jsx)(e.h3,{id:"action-validation",children:"Action Validation"}),"\n",(0,a.jsx)(e.p,{children:"Before executing commands generated by LLMs, implement validation steps:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ActionSafetyValidator:\n    def __init__(self):\n        # Define safety constraints\n        self.forbidden_objects = ["person", "face", "hand", "eye"]  # Objects that shouldn\'t be grabbed\n        self.safe_zones = []  # Define allowed navigation areas\n        self.max_velocity = 0.5  # Restrict robot velocities\n        self.critical_actions = ["open_valve", "close_door"]  # Actions requiring extra validation\n    \n    def validate_action_sequence(self, action_sequence, current_state, environment_state):\n        """Validate an action sequence for safety"""\n        for action in action_sequence:\n            if not self._is_action_safe(action, current_state, environment_state):\n                return False, f"Unsafe action: {action[\'name\']} with parameters {action[\'parameters\']}"\n        \n        return True, "All actions are safe"\n    \n    def _is_action_safe(self, action, current_state, environment_state):\n        """Check if a single action is safe to execute"""\n        action_name = action[\'name\']\n        params = action[\'parameters\']\n        \n        # Check for forbidden object names\n        if \'object\' in params:\n            obj_name = params[\'object\'].lower()\n            if any(forbidden in obj_name for forbidden in self.forbidden_objects):\n                return False\n        \n        # Check navigation targets\n        if action_name == \'move_to_location\' and \'location\' in params:\n            if not self._is_location_safe(params[\'location\'], environment_state):\n                return False\n        \n        # Check critical actions\n        if action_name in self.critical_actions:\n            # Require additional human confirmation for critical actions\n            return False  # For now, block all critical actions\n        \n        return True\n    \n    def _is_location_safe(self, location, environment_state):\n        """Check if a location is in safe zones"""\n        # Implementation would check if location is in predefined safe zones\n        return True\n'})}),"\n",(0,a.jsx)(e.h2,{id:"performance-and-efficiency-optimization",children:"Performance and Efficiency Optimization"}),"\n",(0,a.jsx)(e.h3,{id:"caching",children:"Caching"}),"\n",(0,a.jsx)(e.p,{children:"Cache frequent command patterns to reduce LLM API calls:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from functools import lru_cache\n\nclass CachedCognitivePlanner(LLMBasedCognitivePlanner):\n    @lru_cache(maxsize=100)\n    def plan_with_cache(self, command: str, robot_state_hash: str, env_state_hash: str):\n        """Cached version of planning method"""\n        # Convert hashes back to actual state objects\n        # Call the parent planning method\n        pass\n'})}),"\n",(0,a.jsx)(e.h3,{id:"local-models",children:"Local Models"}),"\n",(0,a.jsx)(e.p,{children:"For privacy and efficiency, consider using local LLM implementations:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from transformers import pipeline\n\nclass LocalCognitivePlanner:\n    def __init__(self, model_name="microsoft/DialoGPT-medium"):\n        self.generator = pipeline(\n            "text-generation",\n            model=model_name,\n            tokenizer=model_name\n        )\n    \n    def plan_local(self, command: str):\n        """Plan using local model"""\n        # Implementation would use local LLM\n        pass\n'})}),"\n",(0,a.jsx)(e.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,a.jsx)(e.h3,{id:"plan-failure-handling",children:"Plan Failure Handling"}),"\n",(0,a.jsx)(e.p,{children:"Implement strategies for when generated plans fail:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class RobustCognitivePlanner:\n    def __init__(self):\n        self.planner = LLMBasedCognitivePlanner()\n        self.validator = ActionSafetyValidator()\n    \n    def execute_with_recovery(self, command: str):\n        """Execute command with error recovery"""\n        max_retries = 3\n        retry_count = 0\n        \n        while retry_count < max_retries:\n            try:\n                # Plan and validate\n                action_seq = self.planner.plan_action_sequence(\n                    command,\n                    self._get_current_state(),\n                    self._get_env_state()\n                )\n                \n                is_safe, reason = self.validator.validate_action_sequence(\n                    action_seq, \n                    self._get_current_state(), \n                    self._get_env_state()\n                )\n                \n                if not is_safe:\n                    raise ValueError(f"Plan unsafe: {reason}")\n                \n                # Execute with monitoring\n                result = self._execute_with_monitoring(action_seq)\n                \n                if result.success:\n                    return result\n                else:\n                    # Plan execution failed, try again with environmental context\n                    retry_count += 1\n                    command = self._refine_command_for_retry(command, result.failure_reason)\n            \n            except Exception as e:\n                retry_count += 1\n                if retry_count >= max_retries:\n                    raise RuntimeError(f"Command failed after {max_retries} attempts: {str(e)}")\n    \n    def _refine_command_for_retry(self, original_command: str, failure_reason: str):\n        """Adjust command based on failure reason"""\n        return f"{original_command}. Note: {failure_reason}. Please adjust your plan accordingly."\n'})}),"\n",(0,a.jsx)(e.h2,{id:"evaluation-and-improvement",children:"Evaluation and Improvement"}),"\n",(0,a.jsx)(e.h3,{id:"metrics-for-cognitive-planning",children:"Metrics for Cognitive Planning"}),"\n",(0,a.jsx)(e.p,{children:"Evaluate the effectiveness of your cognitive planning system:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Task Success Rate"}),": Percentage of commands successfully executed"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Plan Accuracy"}),": How closely generated plans match intended actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Response Time"}),": Time from command to initial action"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"User Satisfaction"}),": Subjective rating of how well the system understood commands"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"continuous-learning",children:"Continuous Learning"}),"\n",(0,a.jsx)(e.p,{children:"Implement feedback mechanisms to improve over time:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class SelfImprovingPlanner:\n    def __init__(self):\n        self.planner = LLMBasedCognitivePlanner()\n        self.feedback_archive = []\n    \n    def record_interaction(self, command, plan, execution_result, human_feedback):\n        """Record interaction for future learning"""\n        interaction = {\n            "command": command,\n            "generated_plan": plan,\n            "execution_result": execution_result,\n            "feedback": human_feedback,\n            "timestamp": time.time()\n        }\n        \n        self.feedback_archive.append(interaction)\n    \n    def refine_with_feedback(self):\n        """Use feedback to improve future planning"""\n        # Implementation would analyze successful patterns in feedback archive\n        pass\n'})}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"LLM-based cognitive planning offers significant potential for natural human-robot interaction by directly translating natural language commands into robot actions. However, successful implementation requires careful attention to safety, validation, and error handling. The approach works best when combined with traditional robotics systems that provide environmental awareness and safety guarantees."}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:'Implement a simple cognitive planner that converts "Bring me the red cup from the kitchen" into a sequence of ROS 2 actions'}),"\n",(0,a.jsx)(e.li,{children:"Add safety validation to prevent the robot from attempting to grasp forbidden objects"}),"\n",(0,a.jsx)(e.li,{children:"Create a feedback mechanism to refine plans based on execution failures"}),"\n",(0,a.jsx)(e.li,{children:"Implement caching for frequently executed commands"}),"\n",(0,a.jsx)(e.li,{children:"Design a multimodal cognitive planner that considers both speech and visual input"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);