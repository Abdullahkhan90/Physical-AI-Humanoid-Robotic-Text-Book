"use strict";(globalThis.webpackChunkai_textbook_docusaurus=globalThis.webpackChunkai_textbook_docusaurus||[]).push([[511],{7883:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var i=r(4848),t=r(8453);const o={sidebar_position:2,title:"Voice-to-Action with OpenAI Whisper"},s="Voice-to-Action: Using OpenAI Whisper for Voice Commands",a={id:"module-4-vla/voice-to-action",title:"Voice-to-Action with OpenAI Whisper",description:"Natural human-machine interaction relies heavily on the ability for humans to communicate with robots using natural language. This section focuses on implementing voice-to-action systems that allow users to control robots using spoken commands, leveraging OpenAI Whisper for voice recognition and interpretation.",source:"@site/docs/module-4-vla/voice-to-action.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/voice-to-action",permalink:"/Physical-AI-Humanoid-Robotic-Text-Book/docs/module-4-vla/voice-to-action",draft:!1,unlisted:!1,editUrl:"https://github.com/Abdullahkhan90/Physical-AI-Humanoid-Robotic-Text-Book/edit/main/docs/module-4-vla/voice-to-action.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Voice-to-Action with OpenAI Whisper"},sidebar:"tutorialSidebar",previous:{title:"Introduction to Vision-Language-Action (VLA)",permalink:"/Physical-AI-Humanoid-Robotic-Text-Book/docs/module-4-vla/intro"},next:{title:"Cognitive Planning with LLMs",permalink:"/Physical-AI-Humanoid-Robotic-Text-Book/docs/module-4-vla/cognitive-planning"}},c={},l=[{value:"Introduction to Voice-to-Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"OpenAI Whisper for Voice Recognition",id:"openai-whisper-for-voice-recognition",level:2},{value:"Overview",id:"overview",level:3},{value:"Whisper Architecture",id:"whisper-architecture",level:3},{value:"Whisper Variants",id:"whisper-variants",level:3},{value:"Implementing Voice Recognition",id:"implementing-voice-recognition",level:2},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Basic Whisper Usage",id:"basic-whisper-usage",level:3},{value:"Real-time Voice Processing",id:"real-time-voice-processing",level:3},{value:"Natural Language Understanding Integration",id:"natural-language-understanding-integration",level:2},{value:"Intent Classification",id:"intent-classification",level:3},{value:"Named Entity Recognition",id:"named-entity-recognition",level:3},{value:"Mapping Commands to Actions",id:"mapping-commands-to-actions",level:2},{value:"Integration with ROS",id:"integration-with-ros",level:2},{value:"Privacy and Security Considerations",id:"privacy-and-security-considerations",level:2},{value:"Data Protection",id:"data-protection",level:3},{value:"Command Validation",id:"command-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Latency Reduction",id:"latency-reduction",level:3},{value:"Accuracy Improvements",id:"accuracy-improvements",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Recognition Failures",id:"recognition-failures",level:3},{value:"Network Resilience",id:"network-resilience",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Recognition Accuracy",id:"recognition-accuracy",level:3},{value:"User Experience",id:"user-experience",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"voice-to-action-using-openai-whisper-for-voice-commands",children:"Voice-to-Action: Using OpenAI Whisper for Voice Commands"}),"\n",(0,i.jsx)(n.p,{children:"Natural human-machine interaction relies heavily on the ability for humans to communicate with robots using natural language. This section focuses on implementing voice-to-action systems that allow users to control robots using spoken commands, leveraging OpenAI Whisper for voice recognition and interpretation."}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-voice-to-action-systems",children:"Introduction to Voice-to-Action Systems"}),"\n",(0,i.jsx)(n.p,{children:"Voice-to-action systems are crucial for natural human-robot interaction. They enable:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Intuitive user interfaces that don't rely on screens or buttons"}),"\n",(0,i.jsx)(n.li,{children:"Hands-free operation for tasks that require manual attention"}),"\n",(0,i.jsx)(n.li,{children:"Accessibility improvements for users with motor impairments"}),"\n",(0,i.jsx)(n.li,{children:"Enhanced productivity in industrial and domestic environments"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The process involves several components:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Voice recognition (converting speech to text)"}),"\n",(0,i.jsx)(n.li,{children:"Natural language understanding (interpreting the command semantics)"}),"\n",(0,i.jsx)(n.li,{children:"Action mapping (translating to robot actions)"}),"\n",(0,i.jsx)(n.li,{children:"Execution and feedback"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"openai-whisper-for-voice-recognition",children:"OpenAI Whisper for Voice Recognition"}),"\n",(0,i.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"OpenAI Whisper is an automatic speech recognition (ASR) system trained on a large dataset of diverse audio. It demonstrates strong performance across multiple languages and accents, making it suitable for multilingual robotics applications."}),"\n",(0,i.jsx)(n.p,{children:"Key features of Whisper:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robustness to accents, background noise, and technical language"}),"\n",(0,i.jsx)(n.li,{children:"Multilingual support (speech-to-text in multiple languages)"}),"\n",(0,i.jsx)(n.li,{children:"Speaker identification capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Time-stamped transcription alignment"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"whisper-architecture",children:"Whisper Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Whisper uses a Transformer-based encoder-decoder architecture:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Audio encoder: Processes mel-scaled spectrograms"}),"\n",(0,i.jsx)(n.li,{children:"Text decoder: Generates text tokens conditional on audio"}),"\n",(0,i.jsx)(n.li,{children:"Tasks: Transcription, translation, language identification"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"whisper-variants",children:"Whisper Variants"}),"\n",(0,i.jsx)(n.p,{children:"Different model sizes offer various trade-offs between accuracy and performance:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"tiny"})," and ",(0,i.jsx)(n.code,{children:"base"}),": Fast, lighter models for real-time or edge applications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"small"})," and ",(0,i.jsx)(n.code,{children:"medium"}),": Balanced between accuracy and performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"large"}),": Most accurate, suitable for demanding applications"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"implementing-voice-recognition",children:"Implementing Voice Recognition"}),"\n",(0,i.jsx)(n.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install Whisper dependencies\r\npip install openai-whisper\r\n# Note: Whisper requires PyTorch and certain system libraries (ffmpeg, etc.)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"basic-whisper-usage",children:"Basic Whisper Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import whisper\r\n\r\n# Load model - specify size based on your requirements\r\nmodel = whisper.load_model("base")\r\n\r\n# Transcribe audio file\r\nresult = model.transcribe("audio.mp3")\r\n\r\n# Access the transcribed text\r\ncommand_text = result["text"]\r\nprint(command_text)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"real-time-voice-processing",children:"Real-time Voice Processing"}),"\n",(0,i.jsx)(n.p,{children:"For real-time applications, you'll need to handle streaming audio:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pyaudio\r\nimport wave\r\nimport whisper\r\nimport numpy as np\r\nfrom threading import Thread\r\nimport queue\r\n\r\nclass WhisperVoiceProcessor:\r\n    def __init__(self, model_size="base"):\r\n        self.model = whisper.load_model(model_size)\r\n        self.command_queue = queue.Queue()\r\n        \r\n        # Audio recording parameters\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.record_seconds = 5\r\n        \r\n    def record_audio(self, filename="temp_record.wav"):\r\n        """Record audio to file for processing"""\r\n        p = pyaudio.PyAudio()\r\n        \r\n        stream = p.open(format=self.format,\r\n                        channels=self.channels,\r\n                        rate=self.rate,\r\n                        input=True,\r\n                        frames_per_buffer=self.chunk)\r\n        \r\n        print("Recording...")\r\n        frames = []\r\n        \r\n        for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\r\n            data = stream.read(self.chunk)\r\n            frames.append(data)\r\n        \r\n        print("Finished recording")\r\n        \r\n        # Stop and close stream\r\n        stream.stop_stream()\r\n        stream.close()\r\n        p.terminate()\r\n        \r\n        # Save recorded audio to WAV file\r\n        wf = wave.open(filename, \'wb\')\r\n        wf.setnchannels(self.channels)\r\n        wf.setsampwidth(p.get_sample_size(self.format))\r\n        wf.setframerate(self.rate)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n        \r\n        return filename\r\n    \r\n    def process_audio(self, audio_file):\r\n        """Process audio file with Whisper"""\r\n        result = self.model.transcribe(audio_file)\r\n        return result["text"]\r\n    \r\n    def listen_and_process(self):\r\n        """Continuously listen for voice commands"""\r\n        while True:\r\n            audio_file = self.record_audio()\r\n            command = self.process_audio(audio_file)\r\n            \r\n            # Add processed command to queue\r\n            self.command_queue.put(command)\r\n            \r\n            # (Optional) delete temporary file\r\n            import os\r\n            os.remove(audio_file)\r\n            \r\n            print(f"Recognized command: {command}")\r\n\r\n# Example usage\r\nprocessor = WhisperVoiceProcessor()\r\n# processor.listen_and_process()  # Start listening\n'})}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-understanding-integration",children:"Natural Language Understanding Integration"}),"\n",(0,i.jsx)(n.p,{children:"After converting speech to text, the system needs to understand the intent:"}),"\n",(0,i.jsx)(n.h3,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,i.jsx)(n.p,{children:"For command-based robotics, you may use classification:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def classify_intent(text):\r\n    """Classify the intent of the voice command"""\r\n    text_lower = text.lower().strip()\r\n    \r\n    # Define command patterns and mappings\r\n    if "move" in text_lower and "forward" in text_lower:\r\n        return {"intent": "MOVE_FORWARD", "params": {}}\r\n    elif "turn" in text_lower and ("left" in text_lower or "right" in text_lower):\r\n        direction = "left" if "left" in text_lower else "right"\r\n        return {"intent": "TURN", "params": {"direction": direction}}\r\n    elif "stop" in text_lower or "halt" in text_lower:\r\n        return {"intent": "STOP", "params": {}}\r\n    elif "pick" in text_lower or "grasp" in text_lower:\r\n        return {"intent": "PICK_UP_OBJECT", "params": {}}\r\n    elif "place" in text_lower or "put" in text_lower:\r\n        return {"intent": "PLACE_OBJECT", "params": {}}\r\n    else:\r\n        return {"intent": "UNKNOWN", "params": {"text": text}}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"named-entity-recognition",children:"Named Entity Recognition"}),"\n",(0,i.jsx)(n.p,{children:"For more complex commands with specific objects or locations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import re\r\nfrom typing import Dict, List\r\n\r\ndef extract_entities(text: str) -> Dict[str, List[str]]:\r\n    """Extract named entities from command text"""\r\n    entities = {\r\n        "objects": [],\r\n        "locations": [],\r\n        "quantities": [],\r\n        "people": []\r\n    }\r\n    \r\n    # Define object patterns\r\n    object_patterns = [\r\n        r"cube|sphere|box|ball|cylinder|cone|pyramid",  # Basic shapes\r\n        r"red|green|blue|yellow|white|black",           # Colors\r\n        r"small|large|medium",                         # Size descriptors\r\n        r"book|cup|plate|apple|banana"                 # Specific objects\r\n    ]\r\n    \r\n    location_patterns = [\r\n        r"kitchen|bedroom|living room|office|bathroom", # Rooms\r\n        r"shelf|table|counter|desk|floor"               # Locations\r\n    ]\r\n    \r\n    # Apply patterns to extract entities\r\n    for pattern in object_patterns:\r\n        matches = re.findall(pattern, text, re.IGNORECASE)\r\n        entities["objects"].extend(matches)\r\n    \r\n    for pattern in location_patterns:\r\n        matches = re.findall(pattern, text, re.IGNORECASE)\r\n        entities["locations"].extend(matches)\r\n    \r\n    # Extract number entities\r\n    quantity_matches = re.findall(r"\\b\\d+\\b", text)\r\n    entities["quantities"] = [int(q) for q in quantity_matches]\r\n    \r\n    return entities\n'})}),"\n",(0,i.jsx)(n.h2,{id:"mapping-commands-to-actions",children:"Mapping Commands to Actions"}),"\n",(0,i.jsx)(n.p,{children:"The system needs to translate understood commands into robot actions:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class CommandMapper:\r\n    def __init__(self):\r\n        self.robot_actions = {\r\n            "MOVE_FORWARD": self.move_forward,\r\n            "TURN": self.turn,\r\n            "STOP": self.stop,\r\n            "PICK_UP_OBJECT": self.pick_up_object,\r\n            "PLACE_OBJECT": self.place_object\r\n        }\r\n    \r\n    def execute_intent(self, intent_result):\r\n        """Execute the action corresponding to the recognized intent"""\r\n        intent = intent_result["intent"]\r\n        params = intent_result["params"]\r\n        \r\n        if intent in self.robot_actions:\r\n            action_func = self.robot_actions[intent]\r\n            return action_func(params)\r\n        else:\r\n            print(f"Unknown intent: {intent}")\r\n            return False\r\n    \r\n    def move_forward(self, params):\r\n        """Move robot forward"""\r\n        # Implementation depends on robot platform\r\n        print("Moving forward")\r\n        # Example: send command via ROS\r\n        # self.move_publisher.publish(Float64(0.5))  # move forward at 0.5 m/s\r\n        return True\r\n    \r\n    def turn(self, params):\r\n        """Turn robot left or right"""\r\n        direction = params.get("direction", "left")\r\n        angle = params.get("angle", 90)  # degrees\r\n        print(f"Turning {direction}")\r\n        return True\r\n    \r\n    def stop(self, params):\r\n        """Stop robot movement"""\r\n        print("Stopping robot")\r\n        return True\r\n    \r\n    def pick_up_object(self, params):\r\n        """Pick up an object"""\r\n        print("Attempting to pick up object")\r\n        # Implementation would involve:\r\n        # 1. Object detection\r\n        # 2. Positioning robot/arm\r\n        # 3. Gripping mechanism activation\r\n        return True\r\n    \r\n    def place_object(self, params):\r\n        """Place object at specified location"""\r\n        location = params.get("location", "table")\r\n        print(f"Placing object at {location}")\r\n        return True\n'})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-ros",children:"Integration with ROS"}),"\n",(0,i.jsx)(n.p,{children:"For integration with ROS-based robot control:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Twist\r\nfrom std_msgs.msg import String\r\nimport queue\r\nimport threading\r\n\r\nclass VoiceControlNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_control_node\')\r\n        \r\n        # Publishers for robot control\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        \r\n        # Queue for incoming voice commands\r\n        self.command_queue = queue.Queue()\r\n        \r\n        # Start voice processing thread\r\n        self.voice_processor = WhisperVoiceProcessor()\r\n        self.processing_thread = threading.Thread(target=self.process_voice_commands)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n        \r\n        # Start command execution timer\r\n        self.timer = self.create_timer(0.1, self.execute_commands)\r\n        \r\n        # Command mapper\r\n        self.command_mapper = CommandMapper()\r\n        \r\n    def process_voice_commands(self):\r\n        """Continuously process voice commands"""\r\n        while True:\r\n            try:\r\n                # Continuously listen for voice commands\r\n                audio_file = self.voice_processor.record_audio()\r\n                command_text = self.voice_processor.process_audio(audio_file)\r\n                \r\n                # Classify intent\r\n                intent_result = classify_intent(command_text)\r\n                \r\n                # Add to execution queue\r\n                self.command_queue.put(intent_result)\r\n                \r\n                # Clean up temp file\r\n                import os\r\n                os.remove(audio_file)\r\n            except Exception as e:\r\n                self.get_logger().error(f"Error processing voice command: {e}")\r\n    \r\n    def execute_commands(self):\r\n        """Execute commands from the queue"""\r\n        try:\r\n            while True:\r\n                intent_result = self.command_queue.get_nowait()\r\n                success = self.command_mapper.execute_intent(intent_result)\r\n                if success:\r\n                    self.get_logger().info(f"Executed command: {intent_result[\'intent\']}")\r\n        except queue.Empty:\r\n            pass  # No commands to process\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    voice_control_node = VoiceControlNode()\r\n    \r\n    try:\r\n        rclpy.spin(voice_control_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        voice_control_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"privacy-and-security-considerations",children:"Privacy and Security Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Voice commands inherently involve capturing audio from the environment, raising important privacy concerns:"}),"\n",(0,i.jsx)(n.h3,{id:"data-protection",children:"Data Protection"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Local processing when possible to avoid sending audio to cloud services"}),"\n",(0,i.jsx)(n.li,{children:"Encrypted storage of any recorded commands for debugging"}),"\n",(0,i.jsx)(n.li,{children:"Clear user consent for data collection and processing"}),"\n",(0,i.jsx)(n.li,{children:"Automatic deletion of audio data after processing"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"command-validation",children:"Command Validation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Authentication for commands that control sensitive functions"}),"\n",(0,i.jsx)(n.li,{children:"Confirmation prompts for critical actions"}),"\n",(0,i.jsx)(n.li,{children:"Validation of command parameters to prevent unintended behavior"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"latency-reduction",children:"Latency Reduction"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Efficient microphone buffering to reduce input latency"}),"\n",(0,i.jsx)(n.li,{children:"Model optimization for faster inference"}),"\n",(0,i.jsx)(n.li,{children:"Parallel processing where possible"}),"\n",(0,i.jsx)(n.li,{children:"Edge computing to reduce network latency"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"accuracy-improvements",children:"Accuracy Improvements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Custom fine-tuning on domain-specific vocabulary"}),"\n",(0,i.jsx)(n.li,{children:"Acoustic model adaptation for specific environments"}),"\n",(0,i.jsx)(n.li,{children:"Speaker adaptation for consistent users"}),"\n",(0,i.jsx)(n.li,{children:"Context-aware language modeling"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,i.jsx)(n.h3,{id:"recognition-failures",children:"Recognition Failures"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Graceful degradation when audio quality is poor"}),"\n",(0,i.jsx)(n.li,{children:"Voice prompts for clarification when command is uncertain"}),"\n",(0,i.jsx)(n.li,{children:"Ability to repeat command or cancel action"}),"\n",(0,i.jsx)(n.li,{children:"Fallback to alternative input methods"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"network-resilience",children:"Network Resilience"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Offline capability for core commands when possible"}),"\n",(0,i.jsx)(n.li,{children:"Caching of recent commands"}),"\n",(0,i.jsx)(n.li,{children:"Graceful failure when network is unavailable"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,i.jsx)(n.p,{children:"To assess the effectiveness of the voice-to-action system:"}),"\n",(0,i.jsx)(n.h3,{id:"recognition-accuracy",children:"Recognition Accuracy"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Word error rate (WER) for transcribed commands"}),"\n",(0,i.jsx)(n.li,{children:"Intent detection accuracy"}),"\n",(0,i.jsx)(n.li,{children:"Entity extraction precision and recall"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"user-experience",children:"User Experience"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Average time to execute command"}),"\n",(0,i.jsx)(n.li,{children:"Number of repetitions needed"}),"\n",(0,i.jsx)(n.li,{children:"User satisfaction ratings"}),"\n",(0,i.jsx)(n.li,{children:"Task completion rates"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"The voice-to-action system using OpenAI Whisper provides an intuitive interface for human-robot interaction. By combining robust voice recognition with natural language understanding, robots can respond to human commands in a more natural way. The implementation requires careful consideration of privacy, performance, and accuracy to create a user-friendly system."}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'Implement a voice-controlled robot that moves forward when commanded "go forward"'}),"\n",(0,i.jsx)(n.li,{children:"Extend the system to recognize and manipulate specific colored objects"}),"\n",(0,i.jsx)(n.li,{children:"Create a multimodal interface that accepts both voice and gesture commands"}),"\n",(0,i.jsx)(n.li,{children:"Implement speaker identification to customize responses based on user identity"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(6540);const t={},o=i.createContext(t);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);