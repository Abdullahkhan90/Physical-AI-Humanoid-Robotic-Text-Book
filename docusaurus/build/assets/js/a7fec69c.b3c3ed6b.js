"use strict";(globalThis.webpackChunkai_textbook_docusaurus=globalThis.webpackChunkai_textbook_docusaurus||[]).push([[511],{7883:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var t=i(4848),o=i(8453);const r={sidebar_position:2,title:"Voice-to-Action with OpenAI Whisper"},s="Voice-to-Action: Using OpenAI Whisper for Voice Commands",a={id:"module-4-vla/voice-to-action",title:"Voice-to-Action with OpenAI Whisper",description:"Natural human-machine interaction relies heavily on the ability for humans to communicate with robots using natural language. This section focuses on implementing voice-to-action systems that allow users to control robots using spoken commands, leveraging OpenAI Whisper for voice recognition and interpretation.",source:"@site/docs/module-4-vla/voice-to-action.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/voice-to-action",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/voice-to-action",draft:!1,unlisted:!1,editUrl:"https://github.com/AI-Textbook-Project/ai-textbook-physical-ai-humanoid/edit/main/docs/module-4-vla/voice-to-action.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Voice-to-Action with OpenAI Whisper"},sidebar:"tutorialSidebar",previous:{title:"Introduction to Vision-Language-Action (VLA)",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/intro"},next:{title:"Cognitive Planning with LLMs",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/cognitive-planning"}},c={},l=[{value:"Introduction to Voice-to-Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"OpenAI Whisper for Voice Recognition",id:"openai-whisper-for-voice-recognition",level:2},{value:"Overview",id:"overview",level:3},{value:"Whisper Architecture",id:"whisper-architecture",level:3},{value:"Whisper Variants",id:"whisper-variants",level:3},{value:"Implementing Voice Recognition",id:"implementing-voice-recognition",level:2},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Basic Whisper Usage",id:"basic-whisper-usage",level:3},{value:"Real-time Voice Processing",id:"real-time-voice-processing",level:3},{value:"Natural Language Understanding Integration",id:"natural-language-understanding-integration",level:2},{value:"Intent Classification",id:"intent-classification",level:3},{value:"Named Entity Recognition",id:"named-entity-recognition",level:3},{value:"Mapping Commands to Actions",id:"mapping-commands-to-actions",level:2},{value:"Integration with ROS",id:"integration-with-ros",level:2},{value:"Privacy and Security Considerations",id:"privacy-and-security-considerations",level:2},{value:"Data Protection",id:"data-protection",level:3},{value:"Command Validation",id:"command-validation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Latency Reduction",id:"latency-reduction",level:3},{value:"Accuracy Improvements",id:"accuracy-improvements",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Recognition Failures",id:"recognition-failures",level:3},{value:"Network Resilience",id:"network-resilience",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Recognition Accuracy",id:"recognition-accuracy",level:3},{value:"User Experience",id:"user-experience",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"voice-to-action-using-openai-whisper-for-voice-commands",children:"Voice-to-Action: Using OpenAI Whisper for Voice Commands"}),"\n",(0,t.jsx)(n.p,{children:"Natural human-machine interaction relies heavily on the ability for humans to communicate with robots using natural language. This section focuses on implementing voice-to-action systems that allow users to control robots using spoken commands, leveraging OpenAI Whisper for voice recognition and interpretation."}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-voice-to-action-systems",children:"Introduction to Voice-to-Action Systems"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems are crucial for natural human-robot interaction. They enable:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Intuitive user interfaces that don't rely on screens or buttons"}),"\n",(0,t.jsx)(n.li,{children:"Hands-free operation for tasks that require manual attention"}),"\n",(0,t.jsx)(n.li,{children:"Accessibility improvements for users with motor impairments"}),"\n",(0,t.jsx)(n.li,{children:"Enhanced productivity in industrial and domestic environments"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The process involves several components:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Voice recognition (converting speech to text)"}),"\n",(0,t.jsx)(n.li,{children:"Natural language understanding (interpreting the command semantics)"}),"\n",(0,t.jsx)(n.li,{children:"Action mapping (translating to robot actions)"}),"\n",(0,t.jsx)(n.li,{children:"Execution and feedback"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"openai-whisper-for-voice-recognition",children:"OpenAI Whisper for Voice Recognition"}),"\n",(0,t.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper is an automatic speech recognition (ASR) system trained on a large dataset of diverse audio. It demonstrates strong performance across multiple languages and accents, making it suitable for multilingual robotics applications."}),"\n",(0,t.jsx)(n.p,{children:"Key features of Whisper:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robustness to accents, background noise, and technical language"}),"\n",(0,t.jsx)(n.li,{children:"Multilingual support (speech-to-text in multiple languages)"}),"\n",(0,t.jsx)(n.li,{children:"Speaker identification capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Time-stamped transcription alignment"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"whisper-architecture",children:"Whisper Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Whisper uses a Transformer-based encoder-decoder architecture:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Audio encoder: Processes mel-scaled spectrograms"}),"\n",(0,t.jsx)(n.li,{children:"Text decoder: Generates text tokens conditional on audio"}),"\n",(0,t.jsx)(n.li,{children:"Tasks: Transcription, translation, language identification"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"whisper-variants",children:"Whisper Variants"}),"\n",(0,t.jsx)(n.p,{children:"Different model sizes offer various trade-offs between accuracy and performance:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"tiny"})," and ",(0,t.jsx)(n.code,{children:"base"}),": Fast, lighter models for real-time or edge applications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"small"})," and ",(0,t.jsx)(n.code,{children:"medium"}),": Balanced between accuracy and performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"large"}),": Most accurate, suitable for demanding applications"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementing-voice-recognition",children:"Implementing Voice Recognition"}),"\n",(0,t.jsx)(n.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install Whisper dependencies\npip install openai-whisper\n# Note: Whisper requires PyTorch and certain system libraries (ffmpeg, etc.)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"basic-whisper-usage",children:"Basic Whisper Usage"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load model - specify size based on your requirements\nmodel = whisper.load_model("base")\n\n# Transcribe audio file\nresult = model.transcribe("audio.mp3")\n\n# Access the transcribed text\ncommand_text = result["text"]\nprint(command_text)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"real-time-voice-processing",children:"Real-time Voice Processing"}),"\n",(0,t.jsx)(n.p,{children:"For real-time applications, you'll need to handle streaming audio:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport wave\nimport whisper\nimport numpy as np\nfrom threading import Thread\nimport queue\n\nclass WhisperVoiceProcessor:\n    def __init__(self, model_size="base"):\n        self.model = whisper.load_model(model_size)\n        self.command_queue = queue.Queue()\n        \n        # Audio recording parameters\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000\n        self.chunk = 1024\n        self.record_seconds = 5\n        \n    def record_audio(self, filename="temp_record.wav"):\n        """Record audio to file for processing"""\n        p = pyaudio.PyAudio()\n        \n        stream = p.open(format=self.format,\n                        channels=self.channels,\n                        rate=self.rate,\n                        input=True,\n                        frames_per_buffer=self.chunk)\n        \n        print("Recording...")\n        frames = []\n        \n        for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n        \n        print("Finished recording")\n        \n        # Stop and close stream\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n        \n        # Save recorded audio to WAV file\n        wf = wave.open(filename, \'wb\')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(p.get_sample_size(self.format))\n        wf.setframerate(self.rate)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n        \n        return filename\n    \n    def process_audio(self, audio_file):\n        """Process audio file with Whisper"""\n        result = self.model.transcribe(audio_file)\n        return result["text"]\n    \n    def listen_and_process(self):\n        """Continuously listen for voice commands"""\n        while True:\n            audio_file = self.record_audio()\n            command = self.process_audio(audio_file)\n            \n            # Add processed command to queue\n            self.command_queue.put(command)\n            \n            # (Optional) delete temporary file\n            import os\n            os.remove(audio_file)\n            \n            print(f"Recognized command: {command}")\n\n# Example usage\nprocessor = WhisperVoiceProcessor()\n# processor.listen_and_process()  # Start listening\n'})}),"\n",(0,t.jsx)(n.h2,{id:"natural-language-understanding-integration",children:"Natural Language Understanding Integration"}),"\n",(0,t.jsx)(n.p,{children:"After converting speech to text, the system needs to understand the intent:"}),"\n",(0,t.jsx)(n.h3,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,t.jsx)(n.p,{children:"For command-based robotics, you may use classification:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def classify_intent(text):\n    """Classify the intent of the voice command"""\n    text_lower = text.lower().strip()\n    \n    # Define command patterns and mappings\n    if "move" in text_lower and "forward" in text_lower:\n        return {"intent": "MOVE_FORWARD", "params": {}}\n    elif "turn" in text_lower and ("left" in text_lower or "right" in text_lower):\n        direction = "left" if "left" in text_lower else "right"\n        return {"intent": "TURN", "params": {"direction": direction}}\n    elif "stop" in text_lower or "halt" in text_lower:\n        return {"intent": "STOP", "params": {}}\n    elif "pick" in text_lower or "grasp" in text_lower:\n        return {"intent": "PICK_UP_OBJECT", "params": {}}\n    elif "place" in text_lower or "put" in text_lower:\n        return {"intent": "PLACE_OBJECT", "params": {}}\n    else:\n        return {"intent": "UNKNOWN", "params": {"text": text}}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"named-entity-recognition",children:"Named Entity Recognition"}),"\n",(0,t.jsx)(n.p,{children:"For more complex commands with specific objects or locations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import re\nfrom typing import Dict, List\n\ndef extract_entities(text: str) -> Dict[str, List[str]]:\n    """Extract named entities from command text"""\n    entities = {\n        "objects": [],\n        "locations": [],\n        "quantities": [],\n        "people": []\n    }\n    \n    # Define object patterns\n    object_patterns = [\n        r"cube|sphere|box|ball|cylinder|cone|pyramid",  # Basic shapes\n        r"red|green|blue|yellow|white|black",           # Colors\n        r"small|large|medium",                         # Size descriptors\n        r"book|cup|plate|apple|banana"                 # Specific objects\n    ]\n    \n    location_patterns = [\n        r"kitchen|bedroom|living room|office|bathroom", # Rooms\n        r"shelf|table|counter|desk|floor"               # Locations\n    ]\n    \n    # Apply patterns to extract entities\n    for pattern in object_patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE)\n        entities["objects"].extend(matches)\n    \n    for pattern in location_patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE)\n        entities["locations"].extend(matches)\n    \n    # Extract number entities\n    quantity_matches = re.findall(r"\\b\\d+\\b", text)\n    entities["quantities"] = [int(q) for q in quantity_matches]\n    \n    return entities\n'})}),"\n",(0,t.jsx)(n.h2,{id:"mapping-commands-to-actions",children:"Mapping Commands to Actions"}),"\n",(0,t.jsx)(n.p,{children:"The system needs to translate understood commands into robot actions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class CommandMapper:\n    def __init__(self):\n        self.robot_actions = {\n            "MOVE_FORWARD": self.move_forward,\n            "TURN": self.turn,\n            "STOP": self.stop,\n            "PICK_UP_OBJECT": self.pick_up_object,\n            "PLACE_OBJECT": self.place_object\n        }\n    \n    def execute_intent(self, intent_result):\n        """Execute the action corresponding to the recognized intent"""\n        intent = intent_result["intent"]\n        params = intent_result["params"]\n        \n        if intent in self.robot_actions:\n            action_func = self.robot_actions[intent]\n            return action_func(params)\n        else:\n            print(f"Unknown intent: {intent}")\n            return False\n    \n    def move_forward(self, params):\n        """Move robot forward"""\n        # Implementation depends on robot platform\n        print("Moving forward")\n        # Example: send command via ROS\n        # self.move_publisher.publish(Float64(0.5))  # move forward at 0.5 m/s\n        return True\n    \n    def turn(self, params):\n        """Turn robot left or right"""\n        direction = params.get("direction", "left")\n        angle = params.get("angle", 90)  # degrees\n        print(f"Turning {direction}")\n        return True\n    \n    def stop(self, params):\n        """Stop robot movement"""\n        print("Stopping robot")\n        return True\n    \n    def pick_up_object(self, params):\n        """Pick up an object"""\n        print("Attempting to pick up object")\n        # Implementation would involve:\n        # 1. Object detection\n        # 2. Positioning robot/arm\n        # 3. Gripping mechanism activation\n        return True\n    \n    def place_object(self, params):\n        """Place object at specified location"""\n        location = params.get("location", "table")\n        print(f"Placing object at {location}")\n        return True\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-ros",children:"Integration with ROS"}),"\n",(0,t.jsx)(n.p,{children:"For integration with ROS-based robot control:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport queue\nimport threading\n\nclass VoiceControlNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_control_node\')\n        \n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        \n        # Queue for incoming voice commands\n        self.command_queue = queue.Queue()\n        \n        # Start voice processing thread\n        self.voice_processor = WhisperVoiceProcessor()\n        self.processing_thread = threading.Thread(target=self.process_voice_commands)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n        \n        # Start command execution timer\n        self.timer = self.create_timer(0.1, self.execute_commands)\n        \n        # Command mapper\n        self.command_mapper = CommandMapper()\n        \n    def process_voice_commands(self):\n        """Continuously process voice commands"""\n        while True:\n            try:\n                # Continuously listen for voice commands\n                audio_file = self.voice_processor.record_audio()\n                command_text = self.voice_processor.process_audio(audio_file)\n                \n                # Classify intent\n                intent_result = classify_intent(command_text)\n                \n                # Add to execution queue\n                self.command_queue.put(intent_result)\n                \n                # Clean up temp file\n                import os\n                os.remove(audio_file)\n            except Exception as e:\n                self.get_logger().error(f"Error processing voice command: {e}")\n    \n    def execute_commands(self):\n        """Execute commands from the queue"""\n        try:\n            while True:\n                intent_result = self.command_queue.get_nowait()\n                success = self.command_mapper.execute_intent(intent_result)\n                if success:\n                    self.get_logger().info(f"Executed command: {intent_result[\'intent\']}")\n        except queue.Empty:\n            pass  # No commands to process\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_control_node = VoiceControlNode()\n    \n    try:\n        rclpy.spin(voice_control_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_control_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"privacy-and-security-considerations",children:"Privacy and Security Considerations"}),"\n",(0,t.jsx)(n.p,{children:"Voice commands inherently involve capturing audio from the environment, raising important privacy concerns:"}),"\n",(0,t.jsx)(n.h3,{id:"data-protection",children:"Data Protection"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Local processing when possible to avoid sending audio to cloud services"}),"\n",(0,t.jsx)(n.li,{children:"Encrypted storage of any recorded commands for debugging"}),"\n",(0,t.jsx)(n.li,{children:"Clear user consent for data collection and processing"}),"\n",(0,t.jsx)(n.li,{children:"Automatic deletion of audio data after processing"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"command-validation",children:"Command Validation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Authentication for commands that control sensitive functions"}),"\n",(0,t.jsx)(n.li,{children:"Confirmation prompts for critical actions"}),"\n",(0,t.jsx)(n.li,{children:"Validation of command parameters to prevent unintended behavior"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"latency-reduction",children:"Latency Reduction"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Efficient microphone buffering to reduce input latency"}),"\n",(0,t.jsx)(n.li,{children:"Model optimization for faster inference"}),"\n",(0,t.jsx)(n.li,{children:"Parallel processing where possible"}),"\n",(0,t.jsx)(n.li,{children:"Edge computing to reduce network latency"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"accuracy-improvements",children:"Accuracy Improvements"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Custom fine-tuning on domain-specific vocabulary"}),"\n",(0,t.jsx)(n.li,{children:"Acoustic model adaptation for specific environments"}),"\n",(0,t.jsx)(n.li,{children:"Speaker adaptation for consistent users"}),"\n",(0,t.jsx)(n.li,{children:"Context-aware language modeling"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,t.jsx)(n.h3,{id:"recognition-failures",children:"Recognition Failures"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Graceful degradation when audio quality is poor"}),"\n",(0,t.jsx)(n.li,{children:"Voice prompts for clarification when command is uncertain"}),"\n",(0,t.jsx)(n.li,{children:"Ability to repeat command or cancel action"}),"\n",(0,t.jsx)(n.li,{children:"Fallback to alternative input methods"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"network-resilience",children:"Network Resilience"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Offline capability for core commands when possible"}),"\n",(0,t.jsx)(n.li,{children:"Caching of recent commands"}),"\n",(0,t.jsx)(n.li,{children:"Graceful failure when network is unavailable"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,t.jsx)(n.p,{children:"To assess the effectiveness of the voice-to-action system:"}),"\n",(0,t.jsx)(n.h3,{id:"recognition-accuracy",children:"Recognition Accuracy"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Word error rate (WER) for transcribed commands"}),"\n",(0,t.jsx)(n.li,{children:"Intent detection accuracy"}),"\n",(0,t.jsx)(n.li,{children:"Entity extraction precision and recall"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"user-experience",children:"User Experience"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Average time to execute command"}),"\n",(0,t.jsx)(n.li,{children:"Number of repetitions needed"}),"\n",(0,t.jsx)(n.li,{children:"User satisfaction ratings"}),"\n",(0,t.jsx)(n.li,{children:"Task completion rates"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"The voice-to-action system using OpenAI Whisper provides an intuitive interface for human-robot interaction. By combining robust voice recognition with natural language understanding, robots can respond to human commands in a more natural way. The implementation requires careful consideration of privacy, performance, and accuracy to create a user-friendly system."}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:'Implement a voice-controlled robot that moves forward when commanded "go forward"'}),"\n",(0,t.jsx)(n.li,{children:"Extend the system to recognize and manipulate specific colored objects"}),"\n",(0,t.jsx)(n.li,{children:"Create a multimodal interface that accepts both voice and gesture commands"}),"\n",(0,t.jsx)(n.li,{children:"Implement speaker identification to customize responses based on user identity"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);