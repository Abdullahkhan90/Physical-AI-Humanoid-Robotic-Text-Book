"use strict";(globalThis.webpackChunkai_textbook_docusaurus=globalThis.webpackChunkai_textbook_docusaurus||[]).push([[581],{5610:i=>{i.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Introduction","items":[{"type":"link","label":"Welcome to the AI Native Textbook on Physical AI & Humanoid Robotics","href":"/physical-ai-humanoid-robotic-text-book/docs/intro","docId":"intro","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/physical-ai-humanoid-robotic-text-book/docs/intro"},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","label":"Introduction to ROS 2","href":"/physical-ai-humanoid-robotic-text-book/docs/module-1-ros2/intro","docId":"module-1-ros2/intro","unlisted":false},{"type":"link","label":"ROS 2 Nodes, Topics, and Services","href":"/physical-ai-humanoid-robotic-text-book/docs/module-1-ros2/nodes-topics-services","docId":"module-1-ros2/nodes-topics-services","unlisted":false},{"type":"link","label":"Integrating Python Agents with ROS Controllers","href":"/physical-ai-humanoid-robotic-text-book/docs/module-1-ros2/integrating-python-agents","docId":"module-1-ros2/integrating-python-agents","unlisted":false},{"type":"link","label":"URDF for Humanoid Robots","href":"/physical-ai-humanoid-robotic-text-book/docs/module-1-ros2/urdf-humanoids","docId":"module-1-ros2/urdf-humanoids","unlisted":false},{"type":"link","label":"Summary and Exercises","href":"/physical-ai-humanoid-robotic-text-book/docs/module-1-ros2/summary-exercises","docId":"module-1-ros2/summary-exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","label":"Introduction to Digital Twins in Robotics","href":"/physical-ai-humanoid-robotic-text-book/docs/module-2-digital-twin/intro","docId":"module-2-digital-twin/intro","unlisted":false},{"type":"link","label":"Physics Simulation in Gazebo","href":"/physical-ai-humanoid-robotic-text-book/docs/module-2-digital-twin/gazebo-physics","docId":"module-2-digital-twin/gazebo-physics","unlisted":false},{"type":"link","label":"Unity Integration for Human-Robot Interaction","href":"/physical-ai-humanoid-robotic-text-book/docs/module-2-digital-twin/unity-integration","docId":"module-2-digital-twin/unity-integration","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","label":"Introduction to AI-Robot Brain with NVIDIA Isaac","href":"/physical-ai-humanoid-robotic-text-book/docs/module-3-ai-brain/intro","docId":"module-3-ai-brain/intro","unlisted":false},{"type":"link","label":"NVIDIA Isaac Sim for Photorealistic Simulation","href":"/physical-ai-humanoid-robotic-text-book/docs/module-3-ai-brain/isaac-sim","docId":"module-3-ai-brain/isaac-sim","unlisted":false},{"type":"link","label":"Isaac ROS for Hardware-Accelerated VSLAM","href":"/physical-ai-humanoid-robotic-text-book/docs/module-3-ai-brain/visual-slam","docId":"module-3-ai-brain/visual-slam","unlisted":false},{"type":"link","label":"Nav2 for Path Planning in Humanoid Movement","href":"/physical-ai-humanoid-robotic-text-book/docs/module-3-ai-brain/nav2-humanoid","docId":"module-3-ai-brain/nav2-humanoid","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","label":"Introduction to Vision-Language-Action (VLA)","href":"/physical-ai-humanoid-robotic-text-book/docs/module-4-vla/intro","docId":"module-4-vla/intro","unlisted":false},{"type":"link","label":"Voice-to-Action with OpenAI Whisper","href":"/physical-ai-humanoid-robotic-text-book/docs/module-4-vla/voice-to-action","docId":"module-4-vla/voice-to-action","unlisted":false},{"type":"link","label":"Cognitive Planning with LLMs","href":"/physical-ai-humanoid-robotic-text-book/docs/module-4-vla/cognitive-planning","docId":"module-4-vla/cognitive-planning","unlisted":false},{"type":"link","label":"Capstone Project - The Autonomous Humanoid","href":"/physical-ai-humanoid-robotic-text-book/docs/module-4-vla/capstone-project","docId":"module-4-vla/capstone-project","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"assets/citations":{"id":"assets/citations","title":"Citations for AI Native Textbook on Physical AI & Humanoid Robotics","description":"References"},"intro":{"id":"intro","title":"Welcome to the AI Native Textbook on Physical AI & Humanoid Robotics","description":"This textbook provides a comprehensive guide to understanding and working with Physical AI and Humanoid Robotics through hands-on experiences in simulated and real-world environments.","sidebar":"tutorialSidebar"},"module-1-ros2/integrating-python-agents":{"id":"module-1-ros2/integrating-python-agents","title":"Integrating Python Agents with ROS Controllers","description":"One of the key strengths of ROS 2 is its support for multiple programming languages. Python, being the language of choice for many AI and machine learning frameworks, becomes an ideal bridge between AI agents and robot control systems. The rclpy client library allows us to interface Python AI agents with ROS controllers.","sidebar":"tutorialSidebar"},"module-1-ros2/intro":{"id":"module-1-ros2/intro","title":"Introduction to ROS 2","description":"Welcome to the first module of our AI Native Textbook on Physical AI & Humanoid Robotics. This module focuses on the Robotic Nervous System, specifically the Robot Operating System 2 (ROS 2).","sidebar":"tutorialSidebar"},"module-1-ros2/nodes-topics-services":{"id":"module-1-ros2/nodes-topics-services","title":"ROS 2 Nodes, Topics, and Services","description":"In ROS 2, nodes communicate through a publish-subscribe model using topics, services, and actions. Understanding these communication patterns is essential for building distributed robotic systems.","sidebar":"tutorialSidebar"},"module-1-ros2/summary-exercises":{"id":"module-1-ros2/summary-exercises","title":"Summary and Exercises","description":"Summary","sidebar":"tutorialSidebar"},"module-1-ros2/urdf-humanoids":{"id":"module-1-ros2/urdf-humanoids","title":"URDF for Humanoid Robots","description":"URDF (Unified Robot Description Format) is an XML format for representing a robot model. URDF is used extensively in ROS to describe robots in terms of their joints, links, visual and collision properties, and more. When designing humanoid robots, URDF plays a crucial role in specifying the physical structure and kinematic properties.","sidebar":"tutorialSidebar"},"module-2-digital-twin/gazebo-physics":{"id":"module-2-digital-twin/gazebo-physics","title":"Physics Simulation in Gazebo","description":"Gazebo is a robotics simulator that provides realistic physics simulation, high-quality graphics, and convenient programmatic interfaces. It is widely used in robotics research and development to simulate robots in complex environments.","sidebar":"tutorialSidebar"},"module-2-digital-twin/intro":{"id":"module-2-digital-twin/intro","title":"Introduction to Digital Twins in Robotics","description":"The digital twin concept involves creating a virtual replica of a physical system. In robotics, this means having a virtual representation of a robot that mirrors its physical counterpart. This module focuses on using Gazebo and Unity for creating realistic physics simulations and environments for robots.","sidebar":"tutorialSidebar"},"module-2-digital-twin/unity-integration":{"id":"module-2-digital-twin/unity-integration","title":"Unity Integration for Human-Robot Interaction","description":"Unity is a powerful cross-platform game engine that has found applications in robotics simulation and human-robot interaction prototyping. Its real-time rendering capabilities and extensive asset ecosystem make it suitable for creating high-fidelity visualizations and user interfaces for robot systems.","sidebar":"tutorialSidebar"},"module-3-ai-brain/intro":{"id":"module-3-ai-brain/intro","title":"Introduction to AI-Robot Brain with NVIDIA Isaac","description":"This module explores the AI-Robot brain, focusing on advanced perception, training, and integration with NVIDIA Isaac platforms. We\'ll learn about NVIDIA Isaac Sim for photorealistic simulation and synthetic data generation, Isaac ROS for hardware-accelerated navigation, and Nav2 for path planning.","sidebar":"tutorialSidebar"},"module-3-ai-brain/isaac-sim":{"id":"module-3-ai-brain/isaac-sim","title":"NVIDIA Isaac Sim for Photorealistic Simulation","description":"NVIDIA Isaac Sim is a robotics simulator based on NVIDIA Omniverse that provides advanced physics simulation capabilities and photorealistic rendering for robotics applications. It enables rapid development and testing of robot applications in virtual environments that closely match real-world conditions.","sidebar":"tutorialSidebar"},"module-3-ai-brain/nav2-humanoid":{"id":"module-3-ai-brain/nav2-humanoid","title":"Nav2 for Path Planning in Humanoid Movement","description":"Navigation 2 (Nav2) is the next-generation navigation system for ROS 2, designed to provide path planning and navigation capabilities for mobile robots. This section focuses on adapting Nav2 for the specific challenges of bipedal humanoid movement, which requires specialized approaches to path planning and locomotion compared to wheeled robots.","sidebar":"tutorialSidebar"},"module-3-ai-brain/visual-slam":{"id":"module-3-ai-brain/visual-slam","title":"Isaac ROS for Hardware-Accelerated VSLAM","description":"Visual Simultaneous Localization and Mapping (VSLAM) is a critical capability for autonomous robots operating in unknown environments. NVIDIA Isaac ROS provides hardware-accelerated VSLAM implementations that leverage GPU acceleration for improved performance and accuracy. This section covers implementing VSLAM systems with Isaac ROS and integrating them into navigation workflows.","sidebar":"tutorialSidebar"},"module-4-vla/capstone-project":{"id":"module-4-vla/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"The capstone project brings together all the concepts learned in previous modules to create an integrated autonomous humanoid robot system. This project involves implementing a simulated humanoid robot that can receive voice commands, plan paths, navigate obstacles, identify objects, and manipulate them based on user instructions.","sidebar":"tutorialSidebar"},"module-4-vla/cognitive-planning":{"id":"module-4-vla/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Large Language Models (LLMs) have emerged as powerful tools for natural language understanding and generation. In robotics, they can serve as high-level cognitive planners that interpret natural language commands and translate them into sequences of executable robot actions. This section explores how to integrate LLMs into robotic systems for cognitive planning.","sidebar":"tutorialSidebar"},"module-4-vla/intro":{"id":"module-4-vla/intro","title":"Introduction to Vision-Language-Action (VLA)","description":"This module explores the convergence of Large Language Models (LLMs) and robotics, specifically focusing on how to bridge high-level language commands with low-level robot actions. We\'ll explore Voice-to-Action systems using OpenAI Whisper and cognitive planning for natural language processing.","sidebar":"tutorialSidebar"},"module-4-vla/voice-to-action":{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action with OpenAI Whisper","description":"Natural human-machine interaction relies heavily on the ability for humans to communicate with robots using natural language. This section focuses on implementing voice-to-action systems that allow users to control robots using spoken commands, leveraging OpenAI Whisper for voice recognition and interpretation.","sidebar":"tutorialSidebar"}}}')}}]);