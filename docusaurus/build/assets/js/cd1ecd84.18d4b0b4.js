"use strict";(globalThis.webpackChunkai_textbook_docusaurus=globalThis.webpackChunkai_textbook_docusaurus||[]).push([[757],{5849:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var o=t(4848),s=t(8453);const i={sidebar_position:5,title:"Capstone Project - The Autonomous Humanoid"},a="Capstone Project: The Autonomous Humanoid",r={id:"module-4-vla/capstone-project",title:"Capstone Project - The Autonomous Humanoid",description:"The capstone project brings together all the concepts learned in previous modules to create an integrated autonomous humanoid robot system. This project involves implementing a simulated humanoid robot that can receive voice commands, plan paths, navigate obstacles, identify objects, and manipulate them based on user instructions.",source:"@site/docs/module-4-vla/capstone-project.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/capstone-project",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/capstone-project",draft:!1,unlisted:!1,editUrl:"https://github.com/AI-Textbook-Project/ai-textbook-physical-ai-humanoid/edit/main/docs/module-4-vla/capstone-project.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,title:"Capstone Project - The Autonomous Humanoid"},sidebar:"tutorialSidebar",previous:{title:"Cognitive Planning with LLMs",permalink:"/ai-textbook-physical-ai-humanoid/docs/module-4-vla/cognitive-planning"}},l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"Implementation Details",id:"implementation-details",level:2},{value:"1. Mission Control Node",id:"1-mission-control-node",level:3},{value:"2. Voice Command Processing Component",id:"2-voice-command-processing-component",level:3},{value:"3. Navigation Controller",id:"3-navigation-controller",level:3},{value:"4. Perception System",id:"4-perception-system",level:3},{value:"5. Manipulation Controller",id:"5-manipulation-controller",level:3},{value:"Project Integration and Testing",id:"project-integration-and-testing",level:2},{value:"Main Project Node",id:"main-project-node",level:3},{value:"Testing Scenarios",id:"testing-scenarios",level:2},{value:"Basic Interaction Test",id:"basic-interaction-test",level:3},{value:"Advanced Scenario Test",id:"advanced-scenario-test",level:3},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Emergency Stop Integration",id:"emergency-stop-integration",level:3},{value:"Performance Metrics",id:"performance-metrics",level:2},{value:"Evaluation Criteria",id:"evaluation-criteria",level:3},{value:"Logging and Analytics",id:"logging-and-analytics",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project brings together all the concepts learned in previous modules to create an integrated autonomous humanoid robot system. This project involves implementing a simulated humanoid robot that can receive voice commands, plan paths, navigate obstacles, identify objects, and manipulate them based on user instructions."}),"\n",(0,o.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,o.jsx)(n.p,{children:"The Autonomous Humanoid capstone project demonstrates the integration of:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 1"}),": ROS 2 communication framework and controller integration"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 2"}),": Simulation environment with Gazebo and perception"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 3"}),": AI-based perception, planning, and navigation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 4"}),": Voice command processing and cognitive planning"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"Upon completion of this project, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate multiple ROS 2 nodes into a cohesive autonomous system"}),"\n",(0,o.jsx)(n.li,{children:"Implement voice control with natural language understanding"}),"\n",(0,o.jsx)(n.li,{children:"Plan navigation paths in complex environments"}),"\n",(0,o.jsx)(n.li,{children:"Execute manipulation tasks based on high-level commands"}),"\n",(0,o.jsx)(n.li,{children:"Demonstrate the ability to interact with the environment and perform tasks based on user instructions"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The autonomous humanoid system architecture consists of these interconnected components:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"[User Voice Command] \n        \u2193\n[Whisper ASR Module] \u2192 [NLP Command Interpreter] \u2192 [Cognitive Planner]\n        \u2193                                             \u2193\n[Mission Control Node] \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2193\n[Navigation System] \u2192 [Path Planner] \u2192 [Footstep Planner] \u2192 [Motion Controller]\n        \u2193                                                     \u2193\n[Perception System] \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2193\n[Object Detection] \u2192 [Manipulation Planner] \u2192 [Arm Controller]\n        \u2193\n[Execution Monitor] \u2192 [Safety Checker] \u2192 [Feedback Generator]\n"})}),"\n",(0,o.jsx)(n.h2,{id:"implementation-details",children:"Implementation Details"}),"\n",(0,o.jsx)(n.h3,{id:"1-mission-control-node",children:"1. Mission Control Node"}),"\n",(0,o.jsx)(n.p,{children:"The central hub that coordinates all system components:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom builtin_interfaces.msg import Duration\nimport queue\nimport threading\nimport time\n\nclass MissionControlNode(Node):\n    def __init__(self):\n        super().__init__('mission_control')\n        \n        # Publishers\n        self.command_pub = self.create_publisher(String, 'missions/command_input', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, 'goal_pose', 10)\n        self.voice_response_pub = self.create_publisher(String, 'voice_response', 10)\n        \n        # Subscribers\n        self.voice_cmd_sub = self.create_subscription(\n            String, 'voice_command', self.voice_command_callback, 10)\n        self.navigation_status_sub = self.create_subscription(\n            String, 'navigation_status', self.nav_status_callback, 10)\n        self.perception_status_sub = self.create_subscription(\n            String, 'perception_status', self.perception_status_callback, 10)\n        self.manipulation_status_sub = self.create_subscription(\n            String, 'manipulation_status', self.manipulation_status_callback, 10)\n        \n        # Internal state\n        self.current_task = None\n        self.task_queue = queue.Queue()\n        self.system_status = {\n            'navigation': 'idle',\n            'perception': 'idle',\n            'manipulation': 'idle',\n            'voice_interface': 'active'\n        }\n        \n        # Timer for system monitoring\n        self.monitor_timer = self.create_timer(1.0, self.system_monitor)\n        \n        # Initialize subsystem controllers\n        self.nav_controller = NavigationController(self)\n        self.perception_controller = PerceptionController(self)\n        self.manipulation_controller = ManipulationController(self)\n        \n        self.get_logger().info(\"Mission Control Node initialized\")\n    \n    def voice_command_callback(self, msg):\n        \"\"\"Process incoming voice commands\"\"\"\n        self.get_logger().info(f\"Received voice command: {msg.data}\")\n        \n        # Parse and validate command\n        parsed_command = self.parse_command(msg.data)\n        if parsed_command:\n            # Add to task queue based on parsed command\n            self.task_queue.put(parsed_command)\n            self.execute_next_task()\n        else:\n            self.respond_to_user(\"I didn't understand that command. Please try again.\")\n    \n    def parse_command(self, command_text):\n        \"\"\"Parse natural language command into structured task\"\"\"\n        command_text = command_text.lower().strip()\n        \n        # Simple command parsing - in a real system, this would be more sophisticated\n        if \"go to\" in command_text and \"kitchen\" in command_text:\n            return {\n                'type': 'NAVIGATION',\n                'destination': 'kitchen',\n                'position': self.get_kitchen_location(),\n                'action': 'navigate'\n            }\n        elif \"pick up\" in command_text or \"grab\" in command_text:\n            object_name = self.extract_object_name(command_text)\n            return {\n                'type': 'MANIPULATION',\n                'action': 'pick_up',\n                'target_object': object_name,\n                'destination': None\n            }\n        elif \"bring\" in command_text and \"me\" in command_text:\n            object_name = self.extract_object_name(command_text)\n            return {\n                'type': 'COMPOSITE',\n                'tasks': [\n                    {'type': 'NAVIGATION', 'destination': 'object_location', 'target_object': object_name},\n                    {'type': 'MANIPULATION', 'action': 'pick_up', 'target_object': object_name},\n                    {'type': 'NAVIGATION', 'destination': 'user_location', 'action': 'return'}\n                ]\n            }\n        elif \"clean\" in command_text or \"tidy\" in command_text:\n            return {\n                'type': 'COMPOSITE',\n                'tasks': [\n                    {'type': 'PERCEPTION', 'action': 'scan_area', 'scan_target': 'room'},\n                    {'type': 'MANIPULATION', 'action': 'collect_items', 'item_types': self.get_cleanable_items()},\n                    {'type': 'NAVIGATION', 'destination': 'disposal_area', 'action': 'deposit_items'}\n                ]\n            }\n        \n        # Default: unrecognized command\n        self.respond_to_user(f\"I don't know how to '{command_text}'. Can you rephrase?\")\n        return None\n    \n    def execute_next_task(self):\n        \"\"\"Execute the next task in the queue\"\"\"\n        if not self.task_queue.empty():\n            task = self.task_queue.get()\n            \n            self.get_logger().info(f\"Executing task: {task['type']}\")\n            \n            if task['type'] == 'NAVIGATION':\n                self.nav_controller.execute_navigation_task(task)\n            elif task['type'] == 'MANIPULATION':\n                self.manipulation_controller.execute_manipulation_task(task)\n            elif task['type'] == 'PERCEPTION':\n                self.perception_controller.execute_perception_task(task)\n            elif task['type'] == 'COMPOSITE':\n                self.execute_composite_task(task)\n    \n    def execute_composite_task(self, composite_task):\n        \"\"\"Execute a sequence of related tasks\"\"\"\n        for subtask in composite_task['tasks']:\n            # Execute each subtask and wait for completion before proceeding\n            self.wait_for_task_completion(subtask)\n    \n    def wait_for_task_completion(self, task):\n        \"\"\"Wait for a specific task to complete\"\"\"\n        # This would involve checking the system status and possibly timeouts\n        timeout = time.time() + 30  # 30 second timeout\n        while self.system_status[self.get_component_for_task(task)] != 'completed':\n            if time.time() > timeout:\n                self.get_logger().error(f\"Task timed out: {task}\")\n                return False\n            time.sleep(0.1)\n        return True\n    \n    def get_component_for_task(self, task):\n        \"\"\"Get the system component responsible for a task\"\"\"\n        if task['type'] == 'NAVIGATION':\n            return 'navigation'\n        elif task['type'] == 'MANIPULATION':\n            return 'manipulation'\n        elif task['type'] == 'PERCEPTION':\n            return 'perception'\n        return 'voice_interface'\n    \n    def nav_status_callback(self, msg):\n        \"\"\"Update navigation status\"\"\"\n        self.system_status['navigation'] = msg.data\n        self.get_logger().info(f\"Navigation status: {msg.data}\")\n    \n    def perception_status_callback(self, msg):\n        \"\"\"Update perception status\"\"\"\n        self.system_status['perception'] = msg.data\n        self.get_logger().info(f\"Perception status: {msg.data}\")\n    \n    def manipulation_status_callback(self, msg):\n        \"\"\"Update manipulation status\"\"\"\n        self.system_status['manipulation'] = msg.data\n        self.get_logger().info(f\"Manipulation status: {msg.data}\")\n    \n    def system_monitor(self):\n        \"\"\"Monitor system health and status\"\"\"\n        # Check if all subsystems are responsive\n        for component, status in self.system_status.items():\n            if status == 'error':\n                self.get_logger().error(f\"{component} system is in error state\")\n                # Handle error condition\n        \n        # Log system status periodically\n        self.get_logger().debug(f\"System status: {self.system_status}\")\n    \n    def respond_to_user(self, response_text):\n        \"\"\"Provide vocal response to user\"\"\"\n        response_msg = String()\n        response_msg.data = response_text\n        self.voice_response_pub.publish(response_msg)\n        self.get_logger().info(f\"Responding to user: {response_text}\")\n    \n    def get_kitchen_location(self):\n        \"\"\"Get pre-defined kitchen location\"\"\"\n        # In a real system, this would come from a map or semantic navigation system\n        pose = PoseStamped()\n        pose.header.frame_id = 'map'\n        pose.pose.position.x = 3.0\n        pose.pose.position.y = 2.0\n        pose.pose.orientation.w = 1.0\n        return pose\n    \n    def extract_object_name(self, command):\n        \"\"\"Extract object name from command (simple implementation)\"\"\"\n        # This is a simplified extraction - a real implementation would use more sophisticated NLP\n        objects = ['cup', 'bottle', 'book', 'toy', 'phone', 'keys', 'glasses', 'hat']\n        for obj in objects:\n            if obj in command:\n                return obj\n        return 'unknown_object'\n    \n    def get_cleanable_items(self):\n        \"\"\"Get list of items that can be cleaned/tidied\"\"\"\n        return ['cup', 'book', 'toy', 'hat', 'glasses']\n"})}),"\n",(0,o.jsx)(n.h3,{id:"2-voice-command-processing-component",children:"2. Voice Command Processing Component"}),"\n",(0,o.jsx)(n.p,{children:"Implementing the voice processing pipeline:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nimport speech_recognition as sr\nimport pyttsx3\nimport threading\n\nclass VoiceInterface:\n    def __init__(self, mission_control_node):\n        self.mission_control = mission_control_node\n        \n        # Initialize speech recognition\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        \n        # Initialize text-to-speech\n        self.tts_engine = pyttsx3.init()\n        self.setup_tts()\n        \n        # Setup Whisper API\n        self.whisper_api_key = self.mission_control.get_parameter_or(\n            \'whisper_api_key\', \'sk-xxxxx\').value\n        \n        # Start listening thread\n        self.listening_active = True\n        self.listening_thread = threading.Thread(target=self.continuous_listening)\n        self.listening_thread.daemon = True\n        self.listening_thread.start()\n    \n    def setup_tts(self):\n        """Configure text-to-speech parameters"""\n        voices = self.tts_engine.getProperty(\'voices\')\n        self.tts_engine.setProperty(\'rate\', 150)  # Speed of speech\n        self.tts_engine.setProperty(\'volume\', 0.9)  # Volume level (0.0 to 1.0)\n        \n        # Try to use female voice if available (often sounds more natural for responses)\n        for voice in voices:\n            if "female" in voice.name.lower() or "zira" in voice.name.lower():\n                self.tts_engine.setProperty(\'voice\', voice.id)\n                break\n    \n    def continuous_listening(self):\n        """Continuously listen for voice commands"""\n        with self.microphone as source:\n            # Adjust for ambient noise\n            self.mission_control.get_logger().info("Adjusting for ambient noise...")\n            self.recognizer.adjust_for_ambient_noise(source)\n            self.mission_control.get_logger().info("Listening for commands...")\n        \n        while self.listening_active:\n            try:\n                # Listen for audio\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5)\n                \n                # Process audio with Whisper\n                command_text = self.process_audio_with_whisper(audio)\n                \n                if command_text:\n                    # Publish command to ROS system\n                    self.publish_command(command_text)\n                \n            except sr.WaitTimeoutError:\n                # Normal - no speech detected within timeout\n                continue\n            except sr.UnknownValueError:\n                # Speech was detected but not understood\n                self.mission_control.get_logger().info("Could not understand audio")\n                self.speak_response("I didn\'t catch that. Could you repeat?")\n            except Exception as e:\n                self.mission_control.get_logger().error(f"Error in voice processing: {e}")\n                self.speak_response("I\'m having trouble listening right now.")\n    \n    def process_audio_with_whisper(self, audio_data):\n        """Process audio using OpenAI Whisper"""\n        try:\n            # Save audio to temporary file\n            with open("/tmp/temp_audio.wav", "wb") as f:\n                f.write(audio_data.get_wav_data())\n            \n            # Use Whisper API to transcribe\n            openai.api_key = self.whisper_api_key\n            with open("/tmp/temp_audio.wav", "rb") as audio_file:\n                transcript = openai.Audio.transcribe("whisper-1", audio_file)\n            \n            command_text = transcript.text.strip()\n            self.mission_control.get_logger().info(f"Transcribed: {command_text}")\n            \n            return command_text\n        except Exception as e:\n            self.mission_control.get_logger().error(f"Whisper processing error: {e}")\n            return None\n    \n    def publish_command(self, command_text):\n        """Publish recognized command to ROS system"""\n        from std_msgs.msg import String\n        cmd_msg = String()\n        cmd_msg.data = command_text\n        self.mission_control.voice_cmd_sub.publish(cmd_msg)\n    \n    def speak_response(self, text):\n        """Speak response to user"""\n        self.mission_control.get_logger().info(f"Speaking response: {text}")\n        \n        def speak_thread():\n            self.tts_engine.say(text)\n            self.tts_engine.runAndWait()\n        \n        # Speak in a separate thread to avoid blocking\n        speak_thread = threading.Thread(target=speak_thread)\n        speak_thread.daemon = True\n        speak_thread.start()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-navigation-controller",children:"3. Navigation Controller"}),"\n",(0,o.jsx)(n.p,{children:"Implementing the path planning and navigation system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nav2_simple_commander.robot_navigator import BasicNavigator\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom std_msgs.msg import String\nimport math\n\nclass NavigationController:\n    def __init__(self, mission_control_node):\n        self.mission_control = mission_control_node\n        self.navigator = BasicNavigator()\n        self.navigator.waitUntilNav2Active()\n        \n        # Publisher for navigation status\n        self.nav_status_pub = self.mission_control.create_publisher(\n            String, 'navigation_status', 10)\n        \n        self.current_destination = None\n        self.path_following_active = False\n    \n    def execute_navigation_task(self, task):\n        \"\"\"Execute a navigation task\"\"\"\n        self.mission_control.get_logger().info(f\"Navigating to: {task['destination']}\")\n        \n        # Update status\n        self.update_navigation_status('navigating')\n        \n        # Create destination pose\n        goal_pose = PoseStamped()\n        goal_pose.header.frame_id = 'map'\n        goal_pose.header.stamp = self.mission_control.get_clock().now().to_msg()\n        \n        if 'position' in task:\n            goal_pose.pose.position.x = task['position'].pose.position.x\n            goal_pose.pose.position.y = task['position'].pose.position.y\n            goal_pose.pose.orientation.w = 1.0\n        else:\n            # Use predefined locations\n            loc = self.get_predefined_location(task['destination'])\n            if loc:\n                goal_pose.pose.position.x = loc[0]\n                goal_pose.pose.position.y = loc[1]\n                goal_pose.pose.orientation.w = loc[2]\n            else:\n                self.mission_control.get_logger().error(f\"Unknown destination: {task['destination']}\")\n                self.update_navigation_status('error')\n                return False\n        \n        # Navigate to goal\n        self.navigator.goToPose(goal_pose)\n        \n        # Monitor navigation progress\n        while not self.navigator.isTaskComplete():\n            # Check for cancellation\n            feedback = self.navigator.getFeedback()\n            if feedback:\n                # Calculate remaining distance\n                current_pose = self.navigator.getRobotPose()\n                remaining_distance = self.calculate_distance(\n                    current_pose.position, \n                    goal_pose.pose.position\n                )\n                \n                # Log progress\n                if remaining_distance < 0.5:\n                    self.mission_control.get_logger().info(\"Approaching destination\")\n        \n        # Check completion result\n        result = self.navigator.getResult()\n        if result == TaskResult.SUCCEEDED:\n            self.mission_control.get_logger().info(f\"Successfully reached {task['destination']}\")\n            self.update_navigation_status('completed')\n            return True\n        else:\n            self.mission_control.get_logger().error(f\"Failed to reach {task['destination']}\")\n            self.update_navigation_status('failed')\n            return False\n    \n    def calculate_distance(self, pos1, pos2):\n        \"\"\"Calculate Euclidean distance between two points\"\"\"\n        dx = pos2.x - pos1.x\n        dy = pos2.y - pos1.y\n        return math.sqrt(dx*dx + dy*dy)\n    \n    def get_predefined_location(self, location_name):\n        \"\"\"Get predefined location coordinates\"\"\"\n        locations = {\n            'kitchen': [3.0, 2.0, 0.0],\n            'living_room': [0.0, 0.0, 0.0],\n            'bedroom': [-2.0, 1.5, 0.0],\n            'office': [1.5, -2.0, 0.0],\n            'dining_room': [2.0, -1.0, 0.0]\n        }\n        return locations.get(location_name.lower())\n    \n    def update_navigation_status(self, status):\n        \"\"\"Update navigation system status\"\"\"\n        status_msg = String()\n        status_msg.data = status\n        self.nav_status_pub.publish(status_msg)\n        \n        # Also update in mission control\n        self.mission_control.system_status['navigation'] = status\n"})}),"\n",(0,o.jsx)(n.h3,{id:"4-perception-system",children:"4. Perception System"}),"\n",(0,o.jsx)(n.p,{children:"Implementing the perception and object detection:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nfrom std_msgs.msg import String\nimport tensorflow as tf\n\nclass PerceptionController:\n    def __init__(self, mission_control_node):\n        self.mission_control = mission_control_node\n        self.cv_bridge = CvBridge()\n        \n        # Subscriber for camera feed\n        self.camera_sub = self.mission_control.create_subscription(\n            Image, '/robot_head_camera/image_raw', self.camera_callback, 10)\n        \n        # Publisher for perception status\n        self.perception_status_pub = self.mission_control.create_publisher(\n            String, 'perception_status', 10)\n        \n        # Load object detection model\n        self.load_detection_model()\n        \n        # Current camera image\n        self.current_image = None\n        self.detection_active = False\n        \n        # Detected objects cache\n        self.detected_objects = []\n    \n    def load_detection_model(self):\n        \"\"\"Load the object detection model\"\"\"\n        # In a real system, this would load a trained model like YOLO or SSD\n        # For this example, we'll simulate detection\n        self.detection_model = None  # Placeholder\n        self.mission_control.get_logger().info(\"Perception model loaded\")\n    \n    def camera_callback(self, msg):\n        \"\"\"Process incoming camera images\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.current_image = cv_image\n            \n            # If detection is active, process the image\n            if self.detection_active:\n                self.process_image_for_detection(cv_image)\n        except Exception as e:\n            self.mission_control.get_logger().error(f\"Camera callback error: {e}\")\n    \n    def process_image_for_detection(self, image):\n        \"\"\"Run object detection on the image\"\"\"\n        # In a real system, this would run the loaded model\n        # For simulation, we'll detect some predefined objects\n        detected_objects = self.simulate_object_detection(image)\n        \n        if detected_objects:\n            self.detected_objects = detected_objects\n            self.mission_control.get_logger().info(f\"Detected objects: {[obj['name'] for obj in detected_objects]}\")\n    \n    def simulate_object_detection(self, image):\n        \"\"\"Simulate object detection for the project\"\"\"\n        # This is a simulation - in a real system, this would use an actual detection model\n        # For demo purposes, we'll return some predefined objects\n        height, width = image.shape[:2]\n        \n        # Predefined objects with positions\n        objects = [\n            {\n                'name': 'red_cup',\n                'bbox': [int(width*0.3), int(height*0.4), int(width*0.4), int(height*0.5)],\n                'confidence': 0.92,\n                'center': (int(width*0.35), int(height*0.45)),\n                'position_3d': self.estimate_3d_position([int(width*0.35), int(height*0.45)])\n            },\n            {\n                'name': 'blue_book',\n                'bbox': [int(width*0.6), int(height*0.5), int(width*0.7), int(height*0.6)],\n                'confidence': 0.87,\n                'center': (int(width*0.65), int(height*0.55)),\n                'position_3d': self.estimate_3d_position([int(width*0.65), int(height*0.55)])\n            }\n        ]\n        \n        return objects\n    \n    def estimate_3d_position(self, pixel_coords):\n        \"\"\"Estimate 3D position from 2D pixel coordinates\"\"\"\n        # Rough estimation - in a real system this would use depth information\n        # For simulation purposes:\n        return {\n            'x': (pixel_coords[0] - 320) * 0.002,  # Approximate conversion from pixels to meters\n            'y': (240 - pixel_coords[1]) * 0.002,  # Invert Y axis\n            'z': 1.0  # Assume object is 1 meter away\n        }\n    \n    def execute_perception_task(self, task):\n        \"\"\"Execute a perception-related task\"\"\"\n        self.mission_control.get_logger().info(f\"Executing perception task: {task['action']}\")\n        \n        self.update_perception_status('processing')\n        \n        if task['action'] == 'scan_area':\n            # Actively scan surroundings for objects\n            self.detection_active = True\n            \n            # Wait for detections (in simulation, we'll just wait a bit)\n            import time\n            time.sleep(2)\n            \n            # Return detection results\n            detection_results = {\n                'objects_found': [obj['name'] for obj in self.detected_objects],\n                'object_details': self.detected_objects\n            }\n            \n            self.mission_control.get_logger().info(f\"Perception results: {detection_results}\")\n            self.update_perception_status('completed')\n            \n            return detection_results\n        elif task['action'] == 'locate_object':\n            # Find a specific object\n            target_obj = task.get('target_object', 'unknown')\n            \n            # Look for object in cached detections\n            for obj in self.detected_objects:\n                if target_obj in obj['name']:\n                    self.mission_control.get_logger().info(f\"Located {target_obj}\")\n                    return obj\n            \n            self.mission_control.get_logger().info(f\"{target_obj} not found in view\")\n            return None\n    \n    def update_perception_status(self, status):\n        \"\"\"Update perception system status\"\"\"\n        status_msg = String()\n        status_msg.data = status\n        self.perception_status_pub.publish(status_msg)\n        \n        # Update mission control\n        self.mission_control.system_status['perception'] = status\n"})}),"\n",(0,o.jsx)(n.h3,{id:"5-manipulation-controller",children:"5. Manipulation Controller"}),"\n",(0,o.jsx)(n.p,{children:"Implementing the manipulation system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom sensor_msgs.msg import JointState\nfrom std_msgs.msg import String\nimport math\n\nclass ManipulationController:\n    def __init__(self, mission_control_node):\n        self.mission_control = mission_control_node\n        \n        # Publishers for manipulation commands\n        self.arm_controller_pub = self.mission_control.create_publisher(\n            JointTrajectory, '/arm_controller/joint_trajectory', 10)\n        self.gripper_controller_pub = self.mission_control.create_publisher(\n            JointTrajectory, '/gripper_controller/joint_trajectory', 10)\n        \n        # Publisher for manipulation status\n        self.manip_status_pub = self.mission_control.create_publisher(\n            String, 'manipulation_status', 10)\n        \n        # Subscriber for joint states\n        self.joint_state_sub = self.mission_control.create_subscription(\n            JointState, '/joint_states', self.joint_state_callback, 10)\n        \n        # Current joint states\n        self.current_joints = {}\n        \n        # Robot kinematics parameters (simplified)\n        self.link_lengths = {\n            'upper_arm': 0.3,\n            'forearm': 0.25\n        }\n    \n    def joint_state_callback(self, msg):\n        \"\"\"Update current joint states\"\"\"\n        for name, position in zip(msg.name, msg.position):\n            self.current_joints[name] = position\n    \n    def execute_manipulation_task(self, task):\n        \"\"\"Execute a manipulation task\"\"\"\n        self.mission_control.get_logger().info(f\"Executing manipulation: {task['action']}\")\n        \n        self.update_manipulation_status('executing')\n        \n        if task['action'] == 'pick_up':\n            # Locate the target object\n            target_obj = task['target_object']\n            \n            # In a real system, this would get the 3D position from perception\n            # For simulation, we'll use a default position\n            target_position = self.get_default_object_position(target_obj)\n            \n            # Plan and execute pick motion\n            success = self.execute_pick_motion(target_position)\n            \n            if success:\n                self.mission_control.get_logger().info(f\"Successfully picked up {target_obj}\")\n                self.update_manipulation_status('completed')\n                return True\n            else:\n                self.mission_control.get_logger().error(f\"Failed to pick up {target_obj}\")\n                self.update_manipulation_status('failed')\n                return False\n        \n        elif task['action'] == 'place':\n            # Place object at specified location\n            place_position = task.get('destination', self.get_default_place_position())\n            success = self.execute_place_motion(place_position)\n            \n            if success:\n                self.mission_control.get_logger().info(\"Object placed successfully\")\n                self.update_manipulation_status('completed')\n                return True\n            else:\n                self.mission_control.get_logger().error(\"Failed to place object\")\n                self.update_manipulation_status('failed')\n                return False\n        \n        return False\n    \n    def get_default_object_position(self, obj_name):\n        \"\"\"Get default position for a known object\"\"\"\n        # In a real system, this would come from perception\n        default_positions = {\n            'cup': {'x': 0.5, 'y': 0.2, 'z': 0.8},\n            'book': {'x': 0.6, 'y': -0.1, 'z': 0.8},\n            'toy': {'x': 0.4, 'y': 0.0, 'z': 0.8}\n        }\n        return default_positions.get(obj_name, {'x': 0.5, 'y': 0.0, 'z': 0.8})\n    \n    def get_default_place_position(self):\n        \"\"\"Get default placement position\"\"\"\n        return {'x': 0.0, 'y': -0.5, 'z': 0.8}  # Place on a surface in front\n    \n    def execute_pick_motion(self, target_position):\n        \"\"\"Execute motion sequence to pick up an object\"\"\"\n        try:\n            # Calculate inverse kinematics\n            joint_angles = self.inverse_kinematics(\n                target_position['x'], \n                target_position['y'], \n                target_position['z']\n            )\n            \n            if joint_angles is None:\n                self.mission_control.get_logger().error(\"IK solution not found\")\n                return False\n            \n            # Open gripper\n            self.open_gripper()\n            \n            # Move to position above object\n            approach_pos = {\n                'x': target_position['x'],\n                'y': target_position['y'],\n                'z': target_position['z'] + 0.1  # 10cm above object\n            }\n            approach_angles = self.inverse_kinematics(\n                approach_pos['x'], approach_pos['y'], approach_pos['z'])\n            \n            if approach_angles:\n                self.move_arm_to_joints(approach_angles)\n            \n            # Move down to object\n            self.move_arm_to_joints(joint_angles)\n            \n            # Close gripper to grasp object\n            self.close_gripper()\n            \n            # Lift object slightly\n            lift_pos = {\n                'x': target_position['x'],\n                'y': target_position['y'],\n                'z': target_position['z'] + 0.15\n            }\n            lift_angles = self.inverse_kinematics(\n                lift_pos['x'], lift_pos['y'], lift_pos['z'])\n            \n            if lift_angles:\n                self.move_arm_to_joints(lift_angles)\n            \n            return True\n            \n        except Exception as e:\n            self.mission_control.get_logger().error(f\"Error in pick motion: {e}\")\n            return False\n    \n    def execute_place_motion(self, place_position):\n        \"\"\"Execute motion sequence to place an object\"\"\"\n        try:\n            # Calculate inverse kinematics for placement position\n            joint_angles = self.inverse_kinematics(\n                place_position['x'], \n                place_position['y'], \n                place_position['z']\n            )\n            \n            if joint_angles is None:\n                self.mission_control.get_logger().error(\"IK solution not found for placement\")\n                return False\n            \n            # Move to placement position\n            self.move_arm_to_joints(joint_angles)\n            \n            # Open gripper to release object\n            self.open_gripper()\n            \n            # Move arm away from placed object\n            retract_pos = {\n                'x': place_position['x'],\n                'y': place_position['y'] - 0.1,  # Move backward\n                'z': place_position['z'] + 0.1   # Move up slightly\n            }\n            retract_angles = self.inverse_kinematics(\n                retract_pos['x'], retract_pos['y'], retract_pos['z'])\n            \n            if retract_angles:\n                self.move_arm_to_joints(retract_angles)\n            \n            return True\n            \n        except Exception as e:\n            self.mission_control.get_logger().error(f\"Error in place motion: {e}\")\n            return False\n    \n    def inverse_kinematics(self, x, y, z):\n        \"\"\"Simple 2D inverse kinematics for planar arm\"\"\"\n        # Simplified 2-joint inverse kinematics\n        # This is a greatly simplified version - real IK would be more complex\n        try:\n            # Calculate distance from origin to target\n            dist_sq = x*x + y*y\n            dist = math.sqrt(dist_sq)\n            \n            # Check reachability\n            arm_length = self.link_lengths['upper_arm'] + self.link_lengths['forearm']\n            if dist > arm_length:\n                return None  # Target not reachable\n            \n            # Calculate joint angles\n            upper_length = self.link_lengths['upper_arm']\n            forearm_length = self.link_lengths['forearm']\n            \n            # Cosine law to find elbow angle\n            cos_angle_elbow = (upper_length*upper_length + forearm_length*forearm_length - dist_sq) / (2 * upper_length * forearm_length)\n            angle_elbow = math.acos(cos_angle_elbow)\n            \n            # Calculate shoulder angle\n            k1 = upper_length + forearm_length * math.cos(angle_elbow)\n            k2 = forearm_length * math.sin(angle_elbow)\n            \n            angle_shoulder = math.atan2(y, x) - math.atan2(k2, k1)\n            \n            # Return joint angles (shoulder, elbow)\n            return [angle_shoulder, math.pi - angle_elbow]\n            \n        except Exception as e:\n            self.mission_control.get_logger().error(f\"IK calculation error: {e}\")\n            return None\n    \n    def move_arm_to_joints(self, joint_angles):\n        \"\"\"Move arm to specified joint angles\"\"\"\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = ['shoulder_joint', 'elbow_joint']  # Simplified\n        \n        point = JointTrajectoryPoint()\n        point.positions = joint_angles\n        point.time_from_start.sec = 2  # 2 seconds to reach position\n        \n        traj_msg.points.append(point)\n        \n        self.arm_controller_pub.publish(traj_msg)\n        \n        # Wait for execution to complete (simplified)\n        import time\n        time.sleep(2.5)\n    \n    def open_gripper(self):\n        \"\"\"Open the gripper\"\"\"\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = ['gripper_joint']\n        \n        point = JointTrajectoryPoint()\n        point.positions = [0.05]  # Open position\n        point.time_from_start.sec = 1\n        \n        traj_msg.points.append(point)\n        self.gripper_controller_pub.publish(traj_msg)\n        \n        import time\n        time.sleep(1.2)\n    \n    def close_gripper(self):\n        \"\"\"Close the gripper to grasp\"\"\"\n        traj_msg = JointTrajectory()\n        traj_msg.joint_names = ['gripper_joint']\n        \n        point = JointTrajectoryPoint()\n        point.positions = [0.01]  # Closed position\n        point.time_from_start.sec = 1\n        \n        traj_msg.points.append(point)\n        self.gripper_controller_pub.publish(traj_msg)\n        \n        import time\n        time.sleep(1.2)\n    \n    def update_manipulation_status(self, status):\n        \"\"\"Update manipulation system status\"\"\"\n        status_msg = String()\n        status_msg.data = status\n        self.manip_status_pub.publish(status_msg)\n        \n        # Update mission control\n        self.mission_control.system_status['manipulation'] = status\n"})}),"\n",(0,o.jsx)(n.h2,{id:"project-integration-and-testing",children:"Project Integration and Testing"}),"\n",(0,o.jsx)(n.h3,{id:"main-project-node",children:"Main Project Node"}),"\n",(0,o.jsx)(n.p,{children:"Combine all components in the main project node:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\ndef main():\n    rclpy.init()\n    \n    # Create mission control node\n    mission_control = MissionControlNode()\n    \n    # Initialize subsystems\n    voice_interface = VoiceInterface(mission_control)\n    nav_controller = NavigationController(mission_control)\n    perception_controller = PerceptionController(mission_control)\n    manipulation_controller = ManipulationController(mission_control)\n    \n    # Start the system\n    mission_control.get_logger().info("Autonomous Humanoid System Online")\n    mission_control.respond_to_user("System is ready. Please give me a command.")\n    \n    try:\n        rclpy.spin(mission_control)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        mission_control.listening_active = False\n        mission_control.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"testing-scenarios",children:"Testing Scenarios"}),"\n",(0,o.jsx)(n.h3,{id:"basic-interaction-test",children:"Basic Interaction Test"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Robot, go to the kitchen"']}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Validates: Voice recognition \u2192 Command parsing \u2192 Navigation"}),"\n",(0,o.jsx)(n.li,{children:"Expected: Robot navigates to kitchen area"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Pick up the red cup"']}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Validates: Perceptio\u2192 Object recognition \u2192 Manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Expected: Robot locates and grasps the cup"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command"}),': "Bring me the cup"']}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Validates: Composite task execution"}),"\n",(0,o.jsx)(n.li,{children:"Expected: Robot retrieves cup and brings it to user"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"advanced-scenario-test",children:"Advanced Scenario Test"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Scenario"}),': "Please clean up the living room by putting books and toys on the shelf"']}),"\n",(0,o.jsx)(n.p,{children:"This complex command should trigger:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Navigation to living room"}),"\n",(0,o.jsx)(n.li,{children:"Area scanning to identify books and toys"}),"\n",(0,o.jsx)(n.li,{children:"Sequential picking up of identified objects"}),"\n",(0,o.jsx)(n.li,{children:"Navigation to shelf location"}),"\n",(0,o.jsx)(n.li,{children:"Placing objects on shelf appropriately"}),"\n",(0,o.jsx)(n.li,{children:"Returning to home position"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,o.jsx)(n.h3,{id:"emergency-stop-integration",children:"Emergency Stop Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class SafetyController:\n    def __init__(self, mission_control_node):\n        self.mission_control = mission_control_node\n        self.emergency_stop_sub = self.mission_control.create_subscription(\n            String, \'emergency_stop\', self.emergency_stop_callback, 10)\n        self.system_armed = True\n        \n    def emergency_stop_callback(self, msg):\n        """Handle emergency stop commands"""\n        if msg.data == \'EMERGENCY_STOP\':\n            self.trigger_emergency_stop()\n    \n    def trigger_emergency_stop(self):\n        """Safely halt all robot motion"""\n        self.mission_control.get_logger().error("EMERGENCY STOP TRIGGERED")\n        self.system_armed = False\n        \n        # Stop all motion\n        self.stop_all_motion()\n        \n        # Move to safe position if possible\n        self.move_to_safe_position()\n        \n    def check_safety_conditions(self):\n        """Check if it\'s safe to continue operations"""\n        # Check for obstacles\n        # Check robot health\n        # Check environment safety\n        return self.system_armed\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,o.jsx)(n.h3,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Completion Rate"}),": Percentage of commands successfully executed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Response Time"}),": Time from command receipt to start of execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation Accuracy"}),": Precision of reaching goal locations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Manipulation Success"}),": Successful grasping and placement ratio"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Recognition Accuracy"}),": Correct understanding of commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"System Recovery"}),": Ability to recover from errors"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"logging-and-analytics",children:"Logging and Analytics"}),"\n",(0,o.jsx)(n.p,{children:"Implement comprehensive logging to track system performance:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class SystemAnalytics:\n    def __init__(self, mission_control_node):\n        self.mission_control = mission_control_node\n        self.session_start_time = None\n        self.commands_processed = 0\n        self.success_count = 0\n        self.error_count = 0\n        \n    def log_command(self, command, result):\n        \"\"\"Log command processing results\"\"\"\n        self.commands_processed += 1\n        if result:\n            self.success_count += 1\n        else:\n            self.error_count += 1\n        \n        # Log to file for analysis\n        self.write_to_logfile(command, result)\n    \n    def get_performance_stats(self):\n        \"\"\"Return current performance metrics\"\"\"\n        total_time = time.time() - self.session_start_time if self.session_start_time else 0\n        \n        return {\n            'total_commands': self.commands_processed,\n            'successful_commands': self.success_count,\n            'error_count': self.error_count,\n            'success_rate': self.success_count / self.commands_processed if self.commands_processed > 0 else 0,\n            'session_duration': total_time,\n            'commands_per_minute': self.commands_processed / (total_time / 60) if total_time > 0 else 0\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"The Autonomous Humanoid capstone project demonstrates the integration of all the concepts covered in this textbook. It showcases how voice commands can be processed through an AI system to generate robot behaviors, how navigation systems guide the robot through environments, how perception systems identify objects, and how manipulation systems interact with the physical world."}),"\n",(0,o.jsx)(n.p,{children:"Successfully completing this project requires:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integration of multiple ROS 2 nodes and subsystems"}),"\n",(0,o.jsx)(n.li,{children:"Proper handling of concurrency and asynchronous operations"}),"\n",(0,o.jsx)(n.li,{children:"Implementation of safety considerations and error recovery"}),"\n",(0,o.jsx)(n.li,{children:"Design of a user-friendly interaction model"}),"\n",(0,o.jsx)(n.li,{children:"Validation of the system in simulated and eventually real environments"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The project serves as a foundation for further exploration in humanoid robotics, with opportunities for enhancement in areas like machine learning, advanced manipulation, improved navigation, and richer human-robot interaction models."}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Enhance the voice command processing to handle more complex multi-step instructions"}),"\n",(0,o.jsx)(n.li,{children:"Implement a learning mechanism that adapts to user preferences over time"}),"\n",(0,o.jsx)(n.li,{children:"Add computer vision object recognition using a trained model (like YOLO)"}),"\n",(0,o.jsx)(n.li,{children:"Implement a more sophisticated navigation system with dynamic obstacle avoidance"}),"\n",(0,o.jsx)(n.li,{children:"Create a GUI interface for non-verbal interaction with the robot"}),"\n",(0,o.jsx)(n.li,{children:"Integrate the system with a mobile base for full mobility"}),"\n",(0,o.jsx)(n.li,{children:"Add tactile feedback to improve manipulation success rates"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const s={},i=o.createContext(s);function a(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);